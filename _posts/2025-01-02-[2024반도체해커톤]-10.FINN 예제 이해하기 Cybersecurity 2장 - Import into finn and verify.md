---
title: "[2024반도체해커톤] 10. FINN 예제 이해하기 Cybersecurity 2장 - Import into finn and verify"
date: 2025-01-02 17:23:00 +09:00
categories: [Project, 2024_2 반도체해커톤,]
tags:
  [
    verilog,
    systemverilog,
    Vivado,
    Vitis,
    Xilinx,
    FINN,
    Bretivas
  ]
---
 
## 환경사항
사용하고 있는 환경은 다음과 같다.

| 이름                      | 버전       | 기타 정보 |
| :-------------------------------| :-----------------| :-------: |
| Vivado			| 2023.2		| O	|
| Vitis			| 2023.2		| O	|
| WSL2			| 5.15.167.4	| O	|
| Ubuntu			| 20.04.06	| O	|
| Docker Desktop		|		| O	|


## 목차 
<a href="https://hyeokls.github.io/posts/2024GC%ED%95%B4%EC%BB%A4%ED%86%A4-0.WLS2-%ED%99%98%EA%B2%BD%EC%97%90%EC%84%9C-%ED%95%98%EB%93%9C%EC%9B%A8%EC%96%B4-%EC%9E%A5%EB%B9%84-%EC%97%B0%EA%B2%B0%ED%95%98%EA%B8%B0/">0. WSL2 환경에서 하드웨어 장비 연결하기<br>
<a href="https://hyeokls.github.io/posts/2024GC%ED%95%B4%EC%BB%A4%ED%86%A4-1.%ED%99%98%EA%B2%BD-%EC%84%A4%EC%A0%95%ED%95%98%EA%B8%B0/">1. 환경 설정하기<br>
<a href="https://hyeokls.github.io/posts/2024GC%ED%95%B4%EC%BB%A4%ED%86%A4-2.FINN-%EC%98%88%EC%A0%9C-%EC%9D%B4%ED%95%B4%ED%95%98%EA%B8%B0-1%ED%8E%B8-How-to-work-with-onnx/">2. FINN 예제 이해하기 1편 Basics - How to used finn<br>
<a href="https://hyeokls.github.io/posts/2024GC%ED%95%B4%EC%BB%A4%ED%86%A4-3.FINN-%EC%98%88%EC%A0%9C-%EC%9D%B4%ED%95%B4%ED%95%98%EA%B8%B0-2%ED%8E%B8-Brevitas-netowrk-import-via-QONNX/">3. FINN 예제 이해하기 2편 - Brevitas network import via QONNX<br>

# 이번 챕터에서는 무엇을 했나요?
이번은 `cybersecurity`챕터에서의 과정을 공부를 진행하였습니다.

---
## 10.0.1. Verify Exported ONNX Model in FINN
---
**FINN에서 내보낸 ONNX 모델 확인하기**

라이브 FINN 튜토리얼: **이 노트북을 읽기 시작할 때 **셀 -> 모두 실행**을 클릭해 “지연 시간 숨기기”를 권장합니다.

**중요: 이 노트북은 거기서 내보낸 ONNX 모델을 사용하기 때문에 1-train-mlp-with-brevitas 노트북에 의존합니다. 따라서 이 노트북을 실행하기 전에 필요한 .onnx 파일이 생성되었는지 확인하세요.**

**또한 Netron 시각화는 동일한 포트를 사용하므로 다른 FINN 노트북을 '닫고 중지'하는 것을 잊지 마세요.**

이 노트북에서는 브레비타스에서 훈련한 네트워크를 가져와 FINN 컴파일러에서 검증하는 방법을 보여드리겠습니다. 

이 검증 프로세스는 실제로 컴파일러의 여러 단계에서 수행할 수 있지만(../bnn-pynq/tfc_end2end_verification.ipynb), 이 예에서는 첫 번째 단계인 내보낸 하이 레벨 FINN-ONNX 모델 검증만 고려하겠습니다.

이 노트북의 또 다른 목표는 *그래프 변환*의 개념을 소개하는 것입니다. 그래프에 몇 가지 변환을 적용하여 검증을 위해 실행 가능하게 만들 것입니다. 

이 모델이 성공적으로 검증되면 다음 노트북에서는 이 모델에서 FPGA 가속기를 생성할 것입니다.

```python
import onnx 
import torch 
```
<br>

---
## 10.0.2. Outline 
---
1. [Import model into FINN with ModelWrapper]
2. [Network preparations: Tidy-up transformations]
3. [Load the dataset and Brevitas model]
4. [Compare FINN and Brevitas execution]


---
## 10.1. Import model into FINN with MOdelWrapper - 1
---
**ModelWrapper를 사용하여 모델을 FINN으로 가져오기**

이제 `.onnx`형식의 모델을 얻었으므로 FINN을 사용하여 작업할 수 있습니다. 

FINN으로 가져오기 위해 [`ModelWrapper`](https://finn.readthedocs.io/en/latest/source_code/finn.core.html#qonnx.core.modelwrapper.ModelWrapper)를 사용합니다. 

이 래퍼는 모델을 더 쉽게 작업할 수 있도록 여러 도우미 함수를 제공하는 ONNX 모델에 대한 래퍼입니다.

```python
import os
from qonnx.core.modelwrapper import ModelWrapper

model_dir = os.environ['FINN_ROOT'] + "/notebooks/end2end_example/cybersecurity"
ready_model_filename = model_dir + "/cybsec-mlp-ready.onnx"
model_for_sim = ModelWrapper(ready_model_filename)
```
<br>

ModelWrapper`가 노출하는 몇 가지 멤버 함수를 살펴보고 어떤 정보를 추출할 수 있는지 알아보겠습니다.

```python
dir(model_for_sim)
```
```bash
#출력
['__class__',
 '__delattr__',
 '__dict__',
 '__dir__',
 '__doc__',
 '__eq__',
 '__format__',
 '__ge__',
 '__getattribute__',
 '__gt__',
 '__hash__',
 '__init__',
 '__init_subclass__',
 '__le__',
 '__lt__',
 '__module__',
 '__ne__',
 '__new__',
 '__reduce__',
 '__reduce_ex__',
 '__repr__',
 '__setattr__',
 '__sizeof__',
 '__str__',
 '__subclasshook__',
 '__weakref__',
 '_model_proto',
 'analysis',
 'check_all_tensor_shapes_specified',
 'check_compatibility',
 'cleanup',
 'find_consumer',
 'find_consumers',
 'find_direct_predecessors',
 'find_direct_successors',
 'find_producer',
 'find_upstream',
 'fix_float64',
 'get_all_tensor_names',
 'get_finn_nodes',
 'get_initializer',
 'get_metadata_prop',
 'get_node_from_name',
 'get_node_index',
 'get_nodes_by_op_type',
 'get_non_finn_nodes',
 'get_tensor_datatype',
 'get_tensor_fanout',
 'get_tensor_layout',
 'get_tensor_shape',
 'get_tensor_sparsity',
 'get_tensor_valueinfo',
 'graph',
 'is_fork_node',
 'is_join_node',
 'make_empty_exec_context',
 'make_new_valueinfo_name',
 'model',
 'rename_tensor',
 'save',
 'set_initializer',
 'set_metadata_prop',
 'set_tensor_datatype',
 'set_tensor_layout',
 'set_tensor_shape',
 'set_tensor_sparsity',
 'temporary_fix_oldstyle_domain',
 'transform']
 ```
 <br>

이러한 도우미 함수의 대부분은 ONNX 모델의 구조 및 속성에 대한 정보를 추출하는 것과 관련이 있습니다. 

프로그래밍 방식으로 ONNX 모델을 검사하고 조작하는 방법에 대한 자세한 내용은 [이 튜토리얼]`../../basics/0_how_to_work_with_onnx.ipynb`에서 찾을 수 있지만 여기서는 몇 가지 기본 함수를 보여드리겠습니다.

예를 들어 그래프에서 다양한 텐서의 모양과 데이터 타입 주석은 물론 각 노드와 연관된 연산 유형과 관련된 정보를 추출할 수 있습니다.

---
## 10.2. Import model into FINN with ModelWrapper - 2
---

```python
from qonnx.core.datatype import DataType

finnonnx_in_tensor_name = model_for_sim.graph.input[0].name
finnonnx_out_tensor_name = model_for_sim.graph.output[0].name
print("Input tensor name: %s" % finnonnx_in_tensor_name)
print("Output tensor name: %s" % finnonnx_out_tensor_name)
finnonnx_model_in_shape = model_for_sim.get_tensor_shape(finnonnx_in_tensor_name)
finnonnx_model_out_shape = model_for_sim.get_tensor_shape(finnonnx_out_tensor_name)
print("Input tensor shape: %s" % str(finnonnx_model_in_shape))
print("Output tensor shape: %s" % str(finnonnx_model_out_shape))
finnonnx_model_in_dt = model_for_sim.get_tensor_datatype(finnonnx_in_tensor_name)
finnonnx_model_out_dt = model_for_sim.get_tensor_datatype(finnonnx_out_tensor_name)
print("Input tensor datatype: %s" % str(finnonnx_model_in_dt.name))
print("Output tensor datatype: %s" % str(finnonnx_model_out_dt.name))
print("List of node operator types in the graph: ")
print([x.op_type for x in model_for_sim.graph.node])
```
```bash
#출력
Input tensor name: global_in
Output tensor name: global_out
Input tensor shape: [1, 600]
Output tensor shape: [1, 1]
Input tensor datatype: BIPOLAR
Output tensor datatype: BIPOLAR
List of node operator types in the graph: 
['Add', 'Div', 'MatMul', 'Mul', 'Add', 'BatchNormalization', 'MultiThreshold', 'Mul', 'MatMul', 'Mul', 'Add', 'BatchNormalization', 'MultiThreshold', 'Mul', 'MatMul', 'Mul', 'Add', 'BatchNormalization', 'MultiThreshold', 'Mul', 'MatMul', 'Mul', 'Add', 'MultiThreshold']
```
<br>

출력 텐서가 바이너리라는 것을 알고 있음에도 불구하고 출력 텐서는 (아직까지는) float32 값으로 표시되어 있다는 점에 유의하세요. 

이는 다음 단계에서 `InferDataTypes` 변환을 실행할 때 컴파일러가 자동으로 추론합니다.

#### 10.2. 이해하기
1. `DataTpye` 모듈을 불러옵니다.

2. `model_for_sim` 의 그래프에서 입력과 출력 텐서의 이름을 가져온 뒤 출력합니다.

3. 입력과 출력 텐서의 형상을 가져온 뒤 출력합니다.

4. 입력과 출력 텐선의 데이터 타입을 가져와 출력합니다.

5. 그래프에 포함된 노드 리스트를 가져와 연산자 유형을 출력합니다.

---
## 10.3. Network preparation: Tidy - up transformations
---
**네트워크 준비: 정리 변환**

검증을 실행하기 전에 Finn-onnx 모델을 준비해야 합니다.

특히 모든 중간 텐서는 정적으로 정의된 모양을 가져야 합니다. 이를 위해 일종의 '정리'와 같은 그래프 변환을 모델에 적용하여 처리하기 쉽도록 합니다. 

**FINN의 그래프 변환.** 

전체 FINN 컴파일러는 모델을 합성 가능한 하드웨어 설명으로 점진적으로 변환하는 변환이라는 개념을 중심으로 구축되었습니다.

FINN은 표준 변환 시퀀스를 자동으로 호출하는 기능을 제공하지만(다음 노트북에서 다룹니다), 여기서처럼 개별 변환을 수동으로 호출하거나 자신만의 변환을 추가하여 사용자 정의 흐름을 만들 수도 있습니다.

이러한 변환에 대한 자세한 내용은 [이 노트북]`../bnn-pynq/tfc_end2end_example.ipynb`에서 확인할 수 있습니다.

```python
from qonnx.transformation.general import GiveReadableTensorNames, GiveUniqueNodeNames, RemoveStaticGraphInputs
from qonnx.transformation.infer_shapes import InferShapes
from qonnx.transformation.infer_datatypes import InferDataTypes
from qonnx.transformation.fold_constants import FoldConstants

model_for_sim = model_for_sim.transform(InferShapes())
model_for_sim = model_for_sim.transform(FoldConstants())
model_for_sim = model_for_sim.transform(GiveUniqueNodeNames())
model_for_sim = model_for_sim.transform(GiveReadableTensorNames())
model_for_sim = model_for_sim.transform(InferDataTypes())
model_for_sim = model_for_sim.transform(RemoveStaticGraphInputs())

verif_model_filename = model_dir + "/cybsec-mlp-verification.onnx"
model_for_sim.save(verif_model_filename)
```

**이렇게 하지 않아도 FINN 컴파일러가 계속 작동할까요?**

다음 노트북의 컴파일 단계에서는 내부적으로 이러한 변환을 적용하여 정상적으로 작동하지만, 아래에서 FINN의 검증 기능을 사용할 예정이므로 깔끔한 변환이 필요합니다.

변환 후 바로 사용할 수 있는 모델을 살펴봅시다.

이제 모든 중간 텐서의 모양이 지정되어 있습니다(레이어 사이를 잇는 화살표 옆에 숫자로 표시됨).

또한, 데이터 유형 추론 단계에서는 양자화 주석을 '다중 임계값' 레이어의 출력(양자화 주석을 보려면 텐서 이름 옆의 +를 클릭하여 확장)과 최종 출력 텐서에 전파했습니다.



-  Onnx 첨부 


#### 10.3. 이해하기
1. `from qonnx.transformation.general`
    - `GiveReadableTensorNames` 는 모든 내부 텐서에 사람이 읽은 수 있는 이름을 부여하는 역학을 합니다.
    
    - `GiveUniqueNodeNames` 는 생성자에서 지정한 접두사로 시작하는 열거형을 사용하여 그래프의 각 노드에 고유한 이름을 지정하는 역할을 합니다.
    
    - `RemoveStaticGraphInputs` 는 `initalizer` 가 있는 최상위 그래프 입력을 모두 제거합니다. 

2. `from qonnx.transformation.infer_shapes` 는 모델의 모든 텐서가 지정된 모양(ValueInfo) 을 가졌는지 확인합니다.

3. `from qonnx.transformation.infer_datatypes` 는 입력 및 노드 유형을 기반으로 모든 중간/출력 텐선에 대한 QONNX 데이터 유형 정보를 추론합니다.

4. `from qonnx.transformation.fold_constants` 는 노드의 출력을 const 전용 입력으로 미리 계산된 결과로 대체합니다.         -
    - exclude_op_types에 지정된 연산자 유형을 건너뜁니다.



---
## 10.4. Load the Dataset and the Brevitas Model - 1 
---
**데이터 세트와 브레비타스 모델 로드하기**

검증을 위한 입력으로 사용하기 위해 이전 노트북에 있는 정량화된 UNSW-NB15 데이터 세트의 몇 가지 예제 데이터를 사용하겠습니다.

```python
import numpy as np
from torch.utils.data import TensorDataset

def get_preqnt_dataset(data_dir: str, train: bool):
    unsw_nb15_data = np.load(data_dir + "/unsw_nb15_binarized.npz")
    if train:
        partition = "train"
    else:
        partition = "test"
    part_data = unsw_nb15_data[partition].astype(np.float32)
    part_data = torch.from_numpy(part_data)
    part_data_in = part_data[:, :-1]
    part_data_out = part_data[:, -1]
    return TensorDataset(part_data_in, part_data_out)

n_verification_inputs = 100
test_quantized_dataset = get_preqnt_dataset(".", False)
input_tensor = test_quantized_dataset.tensors[0][:n_verification_inputs]
input_tensor.shape
```
```bash
#출력
torch.Size([100, 593])
```
<br>

#### 10.4. 이해하기
이 파트는 `.npz` 파일을 읽어 데이터를 처리하여 `PyTorch` 텐서 데이터 셋으로 변환하는 작업입니다.
1. `numpy` 및 `torch.utils.data` 모듈을 가져옵니다.

2. 데이터를 불러와 파이토치 텐서 데이터 셋으로 변환하는 함수를 만들었습니다.
    - 데이터 파일을 불러오고 학습용 및 테스트용을 선택하빈다.

    - 데이터 타입을 float32로 정합니다.

    - Numpy 배열을 PyTorch 텐서로 변환합니다.

    - `TensorDataset`을 반환합니다.

3. 검증에 사용할 입력 데이터의 개수를 정합니다.

4. 텐서를 정하고 출력합니다.
---
## 10.5. Load the Dataset and the Brevitas Model - 2
---

```python
input_size = 593      
hidden1 = 64      
hidden2 = 64
hidden3 = 64
weight_bit_width = 2
act_bit_width = 2
num_classes = 1

from brevitas.nn import QuantLinear, QuantReLU
import torch.nn as nn

brevitas_model = nn.Sequential(
      QuantLinear(input_size, hidden1, bias=True, weight_bit_width=weight_bit_width),
      nn.BatchNorm1d(hidden1),
      nn.Dropout(0.5),
      QuantReLU(bit_width=act_bit_width),
      QuantLinear(hidden1, hidden2, bias=True, weight_bit_width=weight_bit_width),
      nn.BatchNorm1d(hidden2),
      nn.Dropout(0.5),
      QuantReLU(bit_width=act_bit_width),
      QuantLinear(hidden2, hidden3, bias=True, weight_bit_width=weight_bit_width),
      nn.BatchNorm1d(hidden3),
      nn.Dropout(0.5),
      QuantReLU(bit_width=act_bit_width),
      QuantLinear(hidden3, num_classes, bias=True, weight_bit_width=weight_bit_width)
)

# replace this with your trained network checkpoint if you're not
# using the pretrained weights
trained_state_dict = torch.load(model_dir + "/state_dict.pth")["models_state_dict"][0]

# Uncomment the following line if you previously chose to train the network yourself
#trained_state_dict = torch.load("state_dict_self-trained.pth")

brevitas_model.load_state_dict(trained_state_dict, strict=False)
```
```bash
#출력
<All keys matched successfully>
```

#### 10.5. 이해하기
위 코드는 QNN을 구성하는 파트입니다.
1. 파라미터를 정합니다.

2. 라이브러리 모듈을 임포트합니다.

3. 모델을 구축합니다.

4. `trained_state_dict` 을 사용하여 사전에 훈련된 가중치를 불러오고 적용합니다.


### 10.6.1. Load the Dataset and the Brevitas Model - 3


```python
def inference_with_brevitas(current_inp):
    brevitas_output = brevitas_model.forward(current_inp)
    # apply sigmoid + threshold
    brevitas_output = torch.sigmoid(brevitas_output)
    brevitas_output = (brevitas_output.detach().numpy() > 0.5) * 1
    # convert output to bipolar
    brevitas_output = 2*brevitas_output - 1
    return brevitas_output
```

#### 10.6. 이해하기
위 코드는 brevitas 를 통하여 검증을 하는 코드입니다. 

시그모이드, 이진화, Biopolar 등 과정을 거쳐 출력을 반환합니다.


---
## 10.7. Compare FINN & Brevitas execution - 1
---
**FINN & Brevitas 실행 비교**

브레비타스와 FINN에서 동일한 입력을 실행하는 헬퍼 함수를 만들어 보겠습니다.

FINN의 경우, [`finn.core.onnx_exec`](https://finn.readthedocs.io/en/latest/source_code/finn.core.html#finn.core.onnx_exec.execute_onnx) 함수를 사용하여 내보낸 FINN-ONNX를 입력에 실행하겠습니다.

이 ONNX 실행은 검증을 위한 것이지 가속 실행을 위한 것이 아니라는 점에 유의하세요.

데이터 세트의 양자화된 값은 593비트 이진 {0, 1} 벡터인 반면 내보낸 모델은 600비트 바이폴라 {-1, +1} 벡터를 사용하므로 ONNX 모델 검증에 사용하기 전에 약간의 전처리를 거쳐야 한다는 점을 기억하세요.


```python
import finn.core.onnx_exec as oxe

def inference_with_finn_onnx(current_inp):
    finnonnx_in_tensor_name = model_for_sim.graph.input[0].name
    finnonnx_model_in_shape = model_for_sim.get_tensor_shape(finnonnx_in_tensor_name)
    finnonnx_out_tensor_name = model_for_sim.graph.output[0].name
    # convert input to numpy for FINN
    current_inp = current_inp.detach().numpy()
    # add padding and re-scale to bipolar
    current_inp = np.pad(current_inp, [(0, 0), (0, 7)])
    current_inp = 2*current_inp-1
    # reshape to expected input (add 1 for batch dimension)
    current_inp = current_inp.reshape(finnonnx_model_in_shape)
    # create the input dictionary
    input_dict = {finnonnx_in_tensor_name : current_inp} 
    # run with FINN's execute_onnx
    output_dict = oxe.execute_onnx(model_for_sim, input_dict)
    #get the output tensor
    finn_output = output_dict[finnonnx_out_tensor_name] 
    return finn_output
```

#### 10.7. 이해하기
이 코드는 finn_onnx를 검증하는 코드입니다.
1. `finn.core.onnx_exec` 모듈를 임포트합니다.

2. FINN ONNX 모델의 입력 및 출력 텐서 이름을 가져옵니다.
    -입력 텐서의 shape 도 가져옵니다.

3. `current_inp` 를 통하여 입력 데이터를 `numpy` 배열로 변환합니다. 
    - FINN은 PyTorch 텐서를 직접 사용할 수 없기 때문입니다.

4. 입력 데이터에 패딩을 추가하여 입력을 600으로 만들고 데이터를 bipolar 로 변환합니다.

5. 입력 텐서 형상을 맞추기 위해 재배열을 진행합니다.

6. 입력 딕셔너리를 생성합니다.

7. 모델을 실행합니다.

8. 출력 데이터를 추출합니다.


이제 각 입력에 대해 추론 도우미 함수를 호출하고 출력을 비교할 수 있습니다.

---
## 10.8. Compare FINN & Brevitas execution - 2
---

```python
import numpy as np
from tqdm import trange

verify_range = trange(n_verification_inputs, desc="FINN execution", position=0, leave=True)
brevitas_model.eval()

ok = 0
nok = 0

for i in verify_range:
    # run in Brevitas with PyTorch tensor
    current_inp = input_tensor[i].reshape((1, 593))
    brevitas_output = inference_with_brevitas(current_inp)
    finn_output = inference_with_finn_onnx(current_inp)
    # compare the outputs
    ok += 1 if finn_output == brevitas_output else 0
    nok += 1 if finn_output != brevitas_output else 0
    verify_range.set_description("ok %d nok %d" % (ok, nok))
    verify_range.refresh()
```
```bash
#출력
FINN execution:   0%|                                                                           | 0/100 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/_tensor.py:1255: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at ../c10/core/TensorImpl.h:1758.)
  return super(Tensor, self).rename(names)
ok 100 nok 0: 100%|███████████████████████████████████████████████████████████████████| 100/100 [00:14<00:00,  7.04it/s]
```
<br>

```python
try:
    assert ok == n_verification_inputs
    print("Verification succeeded. Brevitas and FINN-ONNX execution outputs are identical")
except AssertionError:
    assert False, "Verification failed. Brevitas and FINN-ONNX execution outputs are NOT identical"
```
```bash
#출력
Verification succeeded. Brevitas and FINN-ONNX execution outputs are identical
```
<br>

#### 10.8.  이해하기
1. `verify_range` 를 통하여 FINN 을 선택합니다.

2. `Brevits_model.eval()` 을 통하여 모델을 평가 모드로 전환합니다.

3. `input_tensor[i]` 를 통하여 인풋 텐서의 **i 번째 입력** 을 가져온 뒤 모델의 형태에 맞게 설정합니다.

4. 각 모델에 대한 추론을 진행합니다.

5. 두 모델의 출력이 같으면 `OK` 를 증가 다르면 `nok` 를 증가시킵니다.

6. 진행이 되고 두 모델이 일치한 수를 비교하여 진행을 확인합니다.




<br>

10.1.1. cybsec-mlp-ready.onnx 를 model_for_sim 으로 래핑
10.2.1. model_for_sim의 구성 및 속성에 대한 정보를 확인
10.3.1. model_for_sim을 Tidy-up 을 진행하였고 이를 cybsec-mlp-verification.onnx 로 저장
10.4.1. `numpy` 및 torch.utils.data` 를 사용하여 `unsw_nb_binarized.npz를  PyTorch의 TensorDataset 객체로 변환
10.5.1. `brevitas` 및 `torch` 를 사용하여 `brevitas_model` 모델을 구성
10.6.1. 구성한 모델에 대하여 `brevitas_output` 을 생성
10.7.1. FINN ONNX 모델을 사용하여 입력 데이터를 실행하고 결과를 얻음
10.8.1. 검증

---