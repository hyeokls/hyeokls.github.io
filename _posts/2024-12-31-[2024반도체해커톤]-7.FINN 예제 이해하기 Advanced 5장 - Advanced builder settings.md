---
title: "[2024GC해커톤] 8.FINN 예제 이해하기 Advanced 5장 - Advanced builder settings"
date: 2024-12-31 17:20:00 +09:00
categories: [Project, 2024_2 반도체해커톤,]
tags:
  [
    verilog,
    systemverilog,
    Vivado,
    Vitis,
    Xilinx,
    FINN,
    Bretivas
  ]
---

## 환경사항
사용하고 있는 환경은 다음과 같다.

| 이름                      | 버전       | 기타 정보 |
| :-------------------------------| :-----------------| :-------: |
| Vivado			| 2023.2		| O	|
| Vitis			| 2023.2		| O	|
| WSL2			| 5.15.167.4	| O	|
| Ubuntu			| 20.04.06	| O	|
| Docker Desktop		|		| O	|


## 목차 
<a href="https://hyeokls.github.io/posts/2024GC%ED%95%B4%EC%BB%A4%ED%86%A4-0.WLS2-%ED%99%98%EA%B2%BD%EC%97%90%EC%84%9C-%ED%95%98%EB%93%9C%EC%9B%A8%EC%96%B4-%EC%9E%A5%EB%B9%84-%EC%97%B0%EA%B2%B0%ED%95%98%EA%B8%B0/">0. WSL2 환경에서 하드웨어 장비 연결하기<br>
<a href="https://hyeokls.github.io/posts/2024GC%ED%95%B4%EC%BB%A4%ED%86%A4-1.%ED%99%98%EA%B2%BD-%EC%84%A4%EC%A0%95%ED%95%98%EA%B8%B0/">1. 환경 설정하기<br>
<a href="https://hyeokls.github.io/posts/2024GC%ED%95%B4%EC%BB%A4%ED%86%A4-2.FINN-%EC%98%88%EC%A0%9C-%EC%9D%B4%ED%95%B4%ED%95%98%EA%B8%B0-1%ED%8E%B8-How-to-work-with-onnx/">2. FINN 예제 이해하기 1편 Basics - How to used finn<br>
<a href="https://hyeokls.github.io/posts/2024GC%ED%95%B4%EC%BB%A4%ED%86%A4-3.FINN-%EC%98%88%EC%A0%9C-%EC%9D%B4%ED%95%B4%ED%95%98%EA%B8%B0-2%ED%8E%B8-Brevitas-netowrk-import-via-QONNX/">3. FINN 예제 이해하기 2편 - Brevitas network import via QONNX<br>

# 이번 챕터에서는 무엇을 했나요?
저번 챕터와 마찬가지로 며칠동안은 FINN에서 제공하는 예제를 이해하는 시간을 가졌습니다.

이번 챕터는 FINN의 변환 패스에 대한 개념을 설명하고 예제를 통해 변환의 절차를 공부할 예정입니다.

---
## 8.0.1. Advanced Builder Settings
---
이 노트북에서는 FINN 컴파일러를 사용하여 CIFAR-10에서 훈련된 소규모 컨볼루션 네트워크에서 스트리밍 데이터 흐름 아키텍처를 갖춘 FPGA 가속기를 생성해 보겠습니다. 

스트리밍 데이터 흐름 아키텍처의 핵심 아이디어는 왼쪽 그림과 같이 각 레이어에 비례하는 양의 컴퓨팅 리소스를 할당하여 레이어 내뿐만 아니라 레이어 간에도 병렬화하는 것입니다. 

일반적인 개념에 대한 자세한 내용은 [FINN](https://arxiv.org/pdf/1612.07119) 및 [FINN-R](https://dl.acm.org/doi/pdf/10.1145/3242897) 백서에서 확인할 수 있습니다. 

이는 각 레이어를 Vitis HLS 또는 RTL 설명에 매핑하고, 각 레이어의 구현을 적절한 수준으로 병렬화하고, 온칩 FIFO를 사용하여 레이어를 연결하여 전체 가속기를 생성함으로써 수행됩니다.


이러한 구현은 성능과 유연성 간에 좋은 균형을 제공하지만 수작업으로 구축하는 것은 어렵고 시간이 많이 소요됩니다. 

이때 FINN 컴파일러를 사용하면 원하는 처리량에 맞게 ONNX 설명에서 스트리밍 데이터 흐름 가속기를 빌드할 수 있습니다.
<br>

이 튜토리얼에서는 FINN 빌더 도구에 대해 자세히 살펴보고 FINN 디자인을 사용자 정의할 수 있는 다양한 옵션을 살펴보겠습니다. 

이미 [사이버 보안 노트북]`../end2end_example/cybersecurity`을 완료했으며 FINN 컴파일러의 작동 방식과 FINN 빌더 도구의 사용법에 대한 기본적인 이해가 있다고 가정합니다.

---
## 8.0.2. OutLine
---

1. [Introduction to the CNV-w2a2 network]
2. [Recap default builder flow]
3. [Build steps]
    1. [How to create a custom build step]
4. [Specialize layers configuration json]
5. [Folding configuration json]
6. [Additional builder arguments]
    1. [Verification steps]
    2. [Other builder arguments]
    3. [Examples for additional builder arguments & bitfile generation]


---
## 8.1. Introduction to the CNV-w2a2 network - 1
---
이 노트북에서 다루게 될 특정 양자화 신경망(QNN)은 CNV-w2a2라고 하며, 32x32 RGB 이미지를 10개의 CIFAR-10 클래스 중 하나로 분류합니다. 

이 네트워크의 모든 가중치와 활성화는 입력(채널당 8비트의 RGB)과 최종 출력(32비트 숫자)을 제외하고 2비트로 양자화됩니다.

이는 [cnv_end2end_example]`../end2end_example/bnn-pynq/cnv_end2end_example.ipynb` Jupyter 노트북에서 사용되는 컨볼루션 신경망과 유사합니다.

Netron에서 네트워크를 구성하는 레이어를 대화형으로 살펴볼 수 있습니다. 

먼저 빌드 디렉터리를 이 노트북이 있는 디렉토리로 설정하고 노트북에서 사용할 도우미 함수를 가져와서 ONNX 그래프와 소스 코드를 살펴봅니다.

```python
from finn.util.visualization import showInNetron, showSrc
import os
    
build_dir = os.environ['FINN_ROOT'] + "/notebooks/advanced"
```
<br>

-오닉스 이미지

---
## 8.2. Introduction to the CNV-w2a2 network - 2 
---


다음 단계에서는 학습된 네트워크를 브레비타스에서 QONNX 형식으로 직접 내보냅니다. 

QONNX는 FINN 컴파일러의 프런트엔드로 사용되는 중간 표현(IR)입니다. 

네트워크의 내부 표현은 여전히 FINN-ONNX 형식이라는 점에 유의하세요. 

[QONNX와 FINN-ONNX](https://finn.readthedocs.io/en/latest/internals.html#intermediate-representation-qonnx-and-finn-onnx)는 특히 8비트 이하의 양자화를 ONNX 그래프에서 표현하기 위한 ONNX 형식의 확장입니다. 

가장 큰 차이점은 QONNX 그래프의 양자화는 전용 양자화 노드([QONNX에 대해 자세히 알아보기](https://github.com/fastmachinelearning/qonnx))를 사용해 표현되는 반면, FINN-ONNX의 양자화는 텐서에 첨부된 주석이라는 점입니다.

```python
import torch
from finn.util.test import get_test_model_trained
from brevitas.export import export_qonnx
from qonnx.util.cleanup import cleanup as qonnx_cleanup

cnv = get_test_model_trained("CNV", 2, 2)
export_onnx_path = build_dir + "/end2end_cnv_w2a2_export.onnx"
export_qonnx(cnv, torch.randn(1, 3, 32, 32), export_onnx_path)
qonnx_cleanup(export_onnx_path, out_file=export_onnx_path)
```
<br>

내보내기 후에는 모델에서 정리 함수를 호출합니다. 

이렇게 하면 예를 들어 네트워크의 모든 모양이 추론되고, 일정한 접기가 적용되며, 모든 텐서와 노드가 고유한 이름을 갖도록 합니다.

다음 단계에서는 Netron을 사용해 그래프를 시각화할 수 있습니다.

그래프를 스크롤하면 네트워크의 양자화를 나타내는 퀀트 노드를 볼 수 있습니다.

FINN 빌더 흐름의 [첫 번째 단계](https://github.com/Xilinx/finn/blob/main/src/finn/builder/build_dataflow_steps.py#L260)에서 네트워크는 QONNX 형식에서 FINN-ONNX 형식으로 변환됩니다. 

즉, 이러한 퀀트 노드는 더 이상 그래프에 존재하지 않고 대신 양자화가 텐서에 주석으로 첨부됩니다.

```python
showInNetron(build_dir+"/end2end_cnv_w2a2_export.onnx")
```
<br>

---
## 8.3. Quick recap, how to setup up default builder flow for resource estimations - 1 
---
간단히 요약하자면, 사이버 보안 예제에서 했던 것처럼 빌더를 설정하여 예제 네트워크에 대한 리소스 추정치를 구해 보겠습니다.

```python
## Quick recap on how to setup the default builder flow for resource estimations

import finn.builder.build_dataflow as build
import finn.builder.build_dataflow_config as build_cfg
import os
import shutil

model_dir = os.environ['FINN_ROOT'] + "/notebooks/advanced"
model_file = model_dir + "/end2end_cnv_w2a2_export.onnx"

estimates_output_dir = build_dir + "/output_estimates_only"

#Delete previous run results if exist
if os.path.exists(estimates_output_dir):
    shutil.rmtree(estimates_output_dir)
    print("Previous run results deleted!")


cfg_estimates = build.DataflowBuildConfig(
    output_dir          = estimates_output_dir,
    mvau_wwidth_max     = 80,
    target_fps          = 10000,
    synth_clk_period_ns = 10.0,
    fpga_part           = "xc7z020clg400-1",
    steps               = build_cfg.estimate_only_dataflow_steps,
    generate_outputs=[
        build_cfg.DataflowOutputType.ESTIMATE_REPORTS,
    ]
)
```
<br>

```bash
%%time
build.build_dataflow_cfg(model_file, cfg_estimates);
```
<br>

출력 디렉터리가 생성되었고 생성된 파일에서 모델에 대한 정보와 FINN 컴파일러에서 처리된 방법을 추출할 수 있습니다.

지금은 중간 모델에 집중해 보겠습니다. 출력 디렉토리의 “intermediate_models” 폴더에서 찾을 수 있습니다.

```bash
!ls -t -r {build_dir}/output_estimates_only/intermediate_models
```

---
## 8.4. Quick recap, how to setup up default builder flow for resource estimations - 2
---
각 FINN 빌더 단계가 끝나면 그래프가 .onnx 파일로 저장됩니다. 

위 셀에서는 빌더 흐름을 시각화하기 위해 시간별로 중간 모델을 내림차순(`ls -t -r`)으로 정렬했습니다. 

FINN-ONNX 형식으로 변환(`STEP_QONNX_TO_FIN`) 후 보시다시피, 그래프는 정리 및 간소화(`STEP_TIDY_UP` 및 `STEP_STREAMLINE`)를 통해 준비되고 상위 레벨 노드는 HW 추상화 레이어로 변환(`STEP_ CONVERT_TO_HW`)됩니다. 

그런 다음 HW 레이어로 변환된 모든 레이어에서 파티션을 생성한 다음(`step_create_dataflow_partition`), 각 HW 추상화 레이어를 HLS 또는 RTL 변형으로 변환합니다(`step_specialize_layers`). 

그 후 최적화를 적용합니다(`step_target_fps_parallelization`, `step_apply_folding_config` 및 `step_minimize_bit_width`). 

이 예제의 마지막 단계에서는 네트워크에 대한 리소스 및 성능 보고서를 생성합니다(`step_generate_estimate_reports`). 

아래 코드를 사용하여 각 단계가 끝난 후 네트워크를 조사하세요.

```python
model_to_investigate = "step_qonnx_to_finn.onnx"
showInNetron(build_dir+"/output_estimates_only/intermediate_models/"+model_to_investigate)
```
<br>

이러한 .onnx 파일을 분석하면 흐름에서 개입해야 할 지점을 파악하고 컴파일러에 추가 정보를 제공하는 데 도움이 될 수 있습니다. 

HW 레이어로 변환된 후 네트워크를 조사하면 변환되지 않은 레이어가 있다는 것을 알 수 있습니다. 

다른 노드를 클릭하면 이를 확인할 수 있습니다. HW 레이어에는 `finn.custom_op.fpgadataflow` 모듈이 있습니다.

```python
showInNetron(build_dir+"/output_estimates_only/intermediate_models/step_convert_to_hw.onnx")
```
<br>

그래프에서 볼 수 있듯이 처음 두 노드(멀티스레숄드 및 트랜스포스 노드)와 마지막 두 노드(뮬 및 추가 노드)는 HW 레이어로 변환되지 않습니다. 

즉, 입력, 출력 및 가중치가 정수로 정량화된 경우에만 노드가 변환된다는 의미로, FINN은 현재 정수 전용 연산만 HW 레이어로 변환합니다.
<br>

그래프에서 'global_in'을 클릭하면 양자화 어노테이션에 데이터 유형이 포함되어 있지 않음을 알 수 있습니다. 

데이터 타입이 설정되어 있지 않고 앞 노드에서 파생할 수 없는 경우, FINN 컴파일러는 자동으로 데이터 타입이 부동소수점이라고 가정합니다.

따라서 첫 번째 노드가 HW 계층으로 변환되지 않고 입력이 부동 소수점으로 간주됩니다.
<br>

문제에 대한 해결책은 실제 데이터 입력에 따라 다릅니다.

1. 데이터 세트가 양자화되고 `global_in`이 정수입니다: ModelWrapper의 헬퍼 함수](https://finn.readthedocs.io/en/latest/internals.html#helper-functions-for-tensors)를 사용하여 모델을 FINN 컴파일러에 전달하기 전에 텐서 `global_in`의 데이터 타입을 설정합니다.

2. 데이터 세트가 정량화되지 않은 경우: 소프트웨어에서 첫 번째 레이어를 실행하거나(예: Python 드라이버의 일부로) 그래프에 전처리 단계를 추가할 수 있습니다.
<br>

CNVw2a2의 예에서 입력은 32x32 RGB 이미지이므로 입력 값은 8비트(UINT8) “양자화”되지만, 내보낸 모델의 입력은 부동 소수점입니다. 

브레비타스에서 학습할 때 이러한 값은 0과 1.0 사이에서 정규화되었으므로 내보낸 모델은 부동 소수점 값을 입력으로 기대합니다. 

즉, 시나리오 2에 해당합니다. 다음 섹션에서는 네트워크에 전처리를 추가하기 위해 FINN 빌더 흐름에 대한 사용자 지정 단계를 개발하겠습니다.

하지만 다음 섹션으로 넘어가기 전에 그래프의 마지막 두 노드 중 HW 레이어로 변환되지 않은 노드를 살펴보겠습니다.
<br>

그래프 끝에 변환할 수 없었던 두 개의 노드, 즉 플로팅 포잉 스칼라 곱셈과 덧셈이 있습니다. 

이러한 연산은 간소화에서 “남은” 연산이며 후속 임계값 연산으로 병합할 수 없습니다. 

이 예제는 이미지 분류를 위한 네트워크이므로 출력은 CIFAR-10 데이터 세트의 각 클래스에 대한 예측 점수를 제공하는 10개의 값으로 구성된 벡터입니다.

분류의 상위 1순위 결과에만 관심이 있다면 그래프에 TopK 노드를 삽입하는 후처리 단계를 추가할 수 있습니다. 

마지막 두 레이어는 스칼라 연산이므로 출력 벡터의 모든 예측 점수에 동일한 영향을 미치므로 안전하게 TopK 노드에 병합할 수 있습니다. 
<br>

이러한 전처리 및 후처리 단계는 네트워크에 따라 다르며, FINN 빌더 도구를 사용하여 실행할 수 있는 **Custom Steps**를 작성해야 합니다.

다음 섹션에서는 먼저 FINN 내부의 표준 빌드 단계가 어떻게 생겼는지 살펴본 다음 전처리 및 후처리를 위한 사용자 정의 단계를 직접 작성하여 빌더 구성에 추가하겠습니다.

---
## 8.5. Build Steps
---
'estimates_only' 플로우를 사용할 때 다음 단계가 실행됩니다.

```python
print("\n".join(build_cfg.estimate_only_dataflow_steps))
```
```bash
#출력
step_qonnx_to_finn
step_tidy_up
step_streamline
step_convert_to_hw
step_create_dataflow_partition
step_specialize_layers
step_target_fps_parallelization
step_apply_folding_config
step_minimize_bit_width
step_generate_estimate_reports
```
<br>

showSrc()` 함수를 사용하거나 문서 문자열에 액세스하여 각 단계를 자세히 살펴볼 수 있습니다.


```python
import finn.builder.build_dataflow_steps as build_dataflow_steps
print(build_dataflow_steps.step_tidy_up.__doc__)
```
```bash
#출력
Run the tidy-up step on given model. This includes shape and datatype
    inference, constant folding, and giving nodes and tensors better names.
```
<br>


```python
import finn.builder.build_dataflow_steps as build_dataflow_steps
showSrc(build_dataflow_steps.step_tidy_up)
```
```bash
def step_tidy_up(model: ModelWrapper, cfg: DataflowBuildConfig):
    """Run the tidy-up step on given model. This includes shape and datatype
    inference, constant folding, and giving nodes and tensors better names.
    """

    model = model.transform(InferShapes())
    model = model.transform(FoldConstants())
    model = model.transform(GiveUniqueNodeNames())
    model = model.transform(GiveReadableTensorNames())
    model = model.transform(InferDataTypes())
    model = model.transform(RemoveStaticGraphInputs())

    if VerificationStepType.TIDY_UP_PYTHON in cfg._resolve_verification_steps():
        verify_step(model, cfg, "initial_python", need_parent=False)

    return model
```
<br>

각 단계는 모델(`model: ModelWrapper`)과 빌드 구성(`cfg: DataflowBuildConfig`)을 입력 인수로 받습니다.

그런 다음 특정 변환 순서가 모델에 적용됩니다.

일부 단계에서는 적용된 변환이 네트워크의 동작을 변경하지 않았는지 확인하기 위해 검증을 실행할 수 있습니다.

마지막으로 수정된 모델이 반환됩니다.

#### 8.5. 이해하기
1. `estimates_only` 플로우에서 실행되는 단계를 의미합니다.
    - step_qonnx_to_finn: QONNX 모델을 FINN 내부 표현으로 변환합니다.

    - step_tidy_up: 모델 구조를 정리하고 최적화합니다. 이 단계에서는 shape 추론, 상수 폴딩, 노드 이름 개선 등이 이루어집니다.

    - step_streamline: 모델 구조를 더욱 최적화하고 단순화합니다.

    - step_convert_to_hw: 소프트웨어 모델을 하드웨어 구현으로 변환합니다. 특정 노드들을 HW 레이어를 나타내는 HWCustomOp로 변환합니다.

    - step_create_dataflow_partition: 모델을 여러 StreamingDataflowPartition으로 분할합니다.

    - step_specialize_layers: 각 레이어에 대해 특정 구현 스타일을 설정합니다.

    - step_target_fps_parallelization: 목표 FPS를 달성하기 위해 모델의 병렬화 수준을 조정합니다.

    - step_apply_folding_config: 폴딩 설정 파일을 모델에 적용하여 병렬화 및 기타 속성을 설정합니다.

    - step_minimize_bit_width: 데이터 표현의 비트 수를 최적화합니다.

    - step_generate_estimate_reports: 리소스 사용량 및 성능에 대한 추정 보고서를 생성합니다.
이 단계들은 실제 하드웨어 구현 없이 모델의 성능과 리소스 사용을 추정하는 데 중점을 둡니다. 

이를 통해 빠르게 다양한 설계 옵션을 탐색하고 평가할 수 있습니다.

2. showSrc 를 통하여 다음과 같이 직접 설명을 볼 수 있습니다.

---
## 8.6. How to create a custom build step - 1
---
**사용자 지정 빌드 단계를 만드는 방법**

자체 사용자 지정 단계를 작성할 때도 동일한 패턴을 사용합니다. 

예제 네트워크의 전처리는 아래 코드를 참조하세요.

```python
from finn.util.pytorch import ToTensor
from qonnx.transformation.merge_onnx_models import MergeONNXModels
from qonnx.core.modelwrapper import ModelWrapper
from qonnx.core.datatype import DataType

def custom_step_add_pre_proc(model: ModelWrapper, cfg: build.DataflowBuildConfig):
    ishape = model.get_tensor_shape(model.graph.input[0].name)
    # preprocessing: torchvision's ToTensor divides uint8 inputs by 255
    preproc = ToTensor()
    export_qonnx(preproc, torch.randn(ishape), "preproc.onnx", opset_version=11)
    preproc_model = ModelWrapper("preproc.onnx")
    # set input finn datatype to UINT8
    preproc_model.set_tensor_datatype(preproc_model.graph.input[0].name, DataType["UINT8"])
    # merge pre-processing onnx model with cnv model (passed as input argument)
    model = model.transform(MergeONNXModels(preproc_model))
    return model
```
<br>

다음 단계에서는 새로 구현된 전처리 사용자 지정 단계를 포함하여 사용자 지정 빌더 단계 시퀀스를 실행하도록 빌더 구성을 수정할 수 있습니다.

이를 위해 `estimate_only` 흐름의 표준 단계 옆에 전처리를 추가하는 새 사용자 지정 단계를 포함하는 `build_steps` 목록을 만듭니다. 

그러면 이 목록이 빌드 구성에 전달됩니다.

```python
## Builder flow with custom step for pre-processing

model_dir = os.environ['FINN_ROOT'] + "/notebooks/advanced"
model_file = model_dir + "/end2end_cnv_w2a2_export.onnx"

output_dir = build_dir + "/output_pre_proc"

#Delete previous run results if exist
if os.path.exists(output_dir):
    shutil.rmtree(output_dir)
    print("Previous run results deleted!")

build_steps = [
    custom_step_add_pre_proc,
    "step_qonnx_to_finn",
    "step_tidy_up",
    "step_streamline",
    "step_convert_to_hw",
    "step_create_dataflow_partition",
    "step_specialize_layers",
    "step_target_fps_parallelization",
    "step_apply_folding_config",
    "step_minimize_bit_width",
    "step_generate_estimate_reports",
]

cfg_estimates = build.DataflowBuildConfig(
    output_dir          = output_dir,
    mvau_wwidth_max     = 80,
    target_fps          = 10000,
    synth_clk_period_ns = 10.0,
    fpga_part           = "xc7z020clg400-1",
    steps               = build_steps,
    generate_outputs=[
        build_cfg.DataflowOutputType.ESTIMATE_REPORTS,
    ]
)
```
<br>

#### 8.6 이해하기
1. 필요 모듈들을 임포트합니다.

2. `def` 문으로 정의를 합니다.
    - `ishape` 으로 모델의 첫 번째 입력 텐서의 shape 을 가져옵니다.

    - PyTorch의 `ToTensor` 을 사용하여 이미지를 텐서로 변환하고 값을 [0,1] 범위로 정규화합니다.

    - `ToTensor` 변환을 ONNX 모델로 내보내고 임의의 입력 텐서를 사용하여 모델 구조를 정의합니다.

    - 내보낸 ONNX 모델을 FINN 의 ModelWrapper 객체로 로드합니다.

    - 전처리 모델의 입력 데이터 타입을 UNINT8 로 설정합니다. 

    - 원본 모델과 전처리 모델을 병합합니다.

    - 모델을 반환합니다.

3. 모델 디렉토리에 관련한 경로를 설정합니다.

4. `if` 문을 통하여 이전 실행 결과가 있다면 삭제를 합니다.

5. `Build_step` 을 통하여 각 단계를 정의합니다.
    - `step_qonnx_to_finn` : 이 단계는 QONNX 노드가 발견된 경우에만 실행되며, QONNX 모델을 FINN-ONNX로 변환합니다.

    - `step_tidy_up` : 이 단계는 주어진 모델에서 정리 단계를 실행하며 추론, 폴딩, 이름 부여 등이 포함됩니다.

    - `step_streamline` : 이 단계는 주어진 모델에서 간소화를 실행하며, 부동 소수점 스케일, 시프트 매개변수를 이동하고 인접한 매개변수를 단일 매개변수로 축소한 다음 스케일/시프트를 다음 `multiTreshold` 노드로 흡수하는 작입이 포함됩니다.

    - `step_convert_to_hw` : 이 단계는 노드를 HW 레이어를 나타내는 `HWCustomOP` 서브클래스로 변환하며 HW로 변환할 수 있는 노드와 특정 구성은 제한되어 있으며, 자세한 내용은 소스를 참고해야합니다.

    - `step_create_dataflow_partition` : 이 단계는 연속된 HWCustomOp 노드 그룹을 `StreamingDataflowPartion` 노드로 분리하여 별도의 ONNX 파일을 가리키며, 데이터 흐름 가속기 합성은 해당 HWCustomOp 하위 그래프에서만 수행할 수 있습니다.

    - `Step_specialize_layers` : 이 단계는 HW 노드를 HLS 또는 RTL 변형 노드로 변환하며, HW 노드는 미리 결정된 규칙에 따라 변환되거나 사용자가 원하는 설정이 포함된 구성 파일을 제공하면 반환됩니다.

    - `step_target_fps_parallelization` : 이 단계는 `target_fps`를 지정한 경우 `SetFolding` 변환을 사용하여 병렬화 속성을 결정하며, 자동 생성된 구성은 출력 아래의 `auto_folding_config.json` 에 저장되며, 이는 폴딩 인자를 추가로 커스터마이징 하는 것에 기초가 될 수 있습니다.

    - `step_apply_folding_config` : 이 단계는 폴딩 구성 파일을 모델에 적용하여 폴딩(병렬화) 및 기타 속성을 설정합니다.(구성 파일이 지정된 경우)

    - `step_minimize_bit_width` : 이 단계는 각 레이어의 가중치 및 누산기 비트 폭을 조정합니다.

    - `step_generate_estimate_reports` : 이 단계는 분석 모델을 사용하여 레이어별 리소스 및 주기 추정치를 생성합니다.

6. `build.DataflowBuildConfig` 객체를 생성하여 빌드 설정을 구성합니다.


---
## 8.7. How to create a custom build step - 2
---
```python
%%time
build.build_dataflow_cfg(model_file, cfg_estimates)
```
```bash
#출력
Building dataflow accelerator from /home/lsh/FPGA/finn/notebooks/advanced/end2end_cnv_w2a2_export.onnx
Intermediate outputs will be generated in /tmp/finn_dev_lsh
Final outputs will be generated in /home/lsh/FPGA/finn/notebooks/advanced/output_pre_proc
Build log is at /home/lsh/FPGA/finn/notebooks/advanced/output_pre_proc/build_dataflow.log
Running step: custom_step_add_pre_proc [1/11]
Running step: step_qonnx_to_finn [2/11]
Running step: step_tidy_up [3/11]
Running step: step_streamline [4/11]
Running step: step_convert_to_hw [5/11]
Running step: step_create_dataflow_partition [6/11]
Running step: step_specialize_layers [7/11]
Running step: step_target_fps_parallelization [8/11]
Running step: step_apply_folding_config [9/11]
Running step: step_minimize_bit_width [10/11]
Running step: step_generate_estimate_reports [11/11]
Completed successfully
CPU times: user 3.6 s, sys: 93.2 ms, total: 3.7 s
Wall time: 3.7 s
```
<br>

```python
!ls -t -r {build_dir}/output_pre_proc/intermediate_models
```
```bash
custom_step_add_pre_proc.onnx  step_create_dataflow_partition.onnx
step_qonnx_to_finn.onnx        step_specialize_layers.onnx
step_tidy_up.onnx	       step_target_fps_parallelization.onnx
step_streamline.onnx	       step_apply_folding_config.onnx
step_convert_to_hw.onnx        step_minimize_bit_width.onnx
supported_op_partitions        step_generate_estimate_reports.onnx
dataflow_parent.onnx
```
<br>

#### 3.7. 이해하기
`build` 를 통하여 빌드를 진행하였고 빌드과정 11가지의 단계에 출력물이 나온 것을 확인할 수 있습니다.
<br>

---
## 8.8. How to create a custom build step - 3
---

사용자 지정 단계를 실행한 후 중간 .onnx 파일이 자동으로 생성되었는데, 그래프를 살펴보겠습니다.

```python
showInNetron(build_dir+"/output_pre_proc/intermediate_models/custom_step_add_pre_proc.onnx")
```

-오닉스 파일

그래프는 QONNX 형식이며 처음에 255로 나눈 값이 삽입되어 있습니다.

이제 그래프의 입력으로 CIFAR-10 이미지를 직접 사용할 수 있으며 새로운 `global_in` 텐서는 UINT8입니다.

위 셀의 코드를 수정하여 중간 모델이 어떻게 변경되었는지 이미 살펴볼 수 있습니다.

더 자세히 살펴보기 전에 후처리를 삽입하기 위해 또 다른 사용자 정의 단계를 추가하겠습니다.

이 경우 이는 TopK 노드를 삽입하는 것을 의미합니다.

```python
from qonnx.transformation.insert_topk import InsertTopK

def custom_step_add_post_proc(model: ModelWrapper, cfg: build.DataflowBuildConfig):
    model = model.transform(InsertTopK(k=1))
    return model
```

#### 8.8. 이해하기
생성된 `.onnx` 파일을 한 번 확인해본 뒤 출력물에 후처리를 진행하기 위해 `TopK` 노드를 추가하려고 하는 내용입니다.

---
## 8.9. How to create a custom build step - 4
---
```python
## Builder flow with custom step for pre-processing and post-processing

model_dir = os.environ['FINN_ROOT'] + "/notebooks/advanced"
model_file = model_dir + "/end2end_cnv_w2a2_export.onnx"

output_dir = build_dir + "/output_pre_and_post_proc"

#Delete previous run results if exist
if os.path.exists(output_dir):
    shutil.rmtree(output_dir)
    print("Previous run results deleted!")

build_steps = [
    custom_step_add_pre_proc,
    custom_step_add_post_proc,
    "step_qonnx_to_finn",
    "step_tidy_up",
    "step_streamline",
    "step_convert_to_hw",
    "step_create_dataflow_partition",
    "step_specialize_layers",
    "step_target_fps_parallelization",
    "step_apply_folding_config",
    "step_minimize_bit_width",
    "step_generate_estimate_reports",
]

cfg_estimates = build.DataflowBuildConfig(
    output_dir          = output_dir,
    mvau_wwidth_max     = 80,
    target_fps          = 10000,
    synth_clk_period_ns = 10.0,
    fpga_part           = "xc7z020clg400-1",
    steps               = build_steps,
    generate_outputs=[
        build_cfg.DataflowOutputType.ESTIMATE_REPORTS,
    ]
)
```
<br>

```python
%%time
build.build_dataflow_cfg(model_file, cfg_estimates);
```
```bash
#출력
Building dataflow accelerator from /home/lsh/FPGA/finn/notebooks/advanced/end2end_cnv_w2a2_export.onnx
Intermediate outputs will be generated in /tmp/finn_dev_lsh
Final outputs will be generated in /home/lsh/FPGA/finn/notebooks/advanced/output_pre_and_post_proc
Build log is at /home/lsh/FPGA/finn/notebooks/advanced/output_pre_and_post_proc/build_dataflow.log
Running step: custom_step_add_pre_proc [1/12]
Running step: custom_step_add_post_proc [2/12]
Running step: step_qonnx_to_finn [3/12]
Running step: step_tidy_up [4/12]
Running step: step_streamline [5/12]
Running step: step_convert_to_hw [6/12]
Running step: step_create_dataflow_partition [7/12]
Running step: step_specialize_layers [8/12]
Running step: step_target_fps_parallelization [9/12]
Running step: step_apply_folding_config [10/12]
Running step: step_minimize_bit_width [11/12]
Running step: step_generate_estimate_reports [12/12]
Completed successfully
CPU times: user 3.69 s, sys: 62.7 ms, total: 3.75 s
Wall time: 3.75 s
```
<br>

```python
!ls -t -r {build_dir}/output_pre_and_post_proc/intermediate_models
```
```bash
#출력
custom_step_add_pre_proc.onnx	step_create_dataflow_partition.onnx
custom_step_add_post_proc.onnx	dataflow_parent.onnx
step_qonnx_to_finn.onnx		step_specialize_layers.onnx
step_tidy_up.onnx		step_target_fps_parallelization.onnx
step_streamline.onnx		step_apply_folding_config.onnx
supported_op_partitions		step_minimize_bit_width.onnx
step_convert_to_hw.onnx		step_generate_estimate_reports.onnx
```
<br>

```python
model_to_investigate = "custom_step_add_post_proc.onnx"
showInNetron(build_dir+"/output_pre_and_post_proc/intermediate_models/"+model_to_investigate)
```
- 오닉스 출력

이제 모든 레이어가 올바르게 변환되었는지 확인하기 위해 hw로 변환한 후 모델을 살펴 보겠습니다.

```python
showInNetron(build_dir+"/output_pre_and_post_proc/intermediate_models/step_convert_to_hw.onnx")
```

- 오닉스

이제 모델에는 시작 부분에 `Thresholding` 레이어가 있고 끝 부분에 `LabelSelect` 레이어가 있습니다.

그래프의 첫 번째 레이어에 여전히 `Transpose` 노드가 있지만, 입력 데이터를 FINN 가속기로 스트리밍하기 전에 NHWC 형식으로 변환하여 이 문제를 해결할 수 있습니다.


#### 8.9. 이해하기
`8.6.1. How to create a custom build step - 1` 에서 진행했던 부분에 후처리 과정만 추가된 코드이며 방식은 동일합니다. 

단계는 1단계가 추가되어 총 12 단계이며 출력물도 그에 따라 12개인 것을 확인할 수 있습니다.

또한 첫 단계인 `add_post_proc.onnx` 을 확인해본 뒤 `convert_to_hw.onxx` 도 확인하여 검토를 진행합니다.

---
## 8.10. Specialize layers configuration json - 1
---
**레이어 구성 Json 특화?**
##### 1. 
FINN 컴파일러는 신경망 레이어에 해당하는 하드웨어 블록이 HLS를 기반으로 개발된다는 가정 하에 개발되었습니다.

현재로서는 이 HLS 구현을 폐지하고 싶지는 않지만, 특정 모듈의 경우 RTL로 구현하는 것이 합리적이라는 것이 수년에 걸쳐 분명해졌습니다. 

이를 통해 결과 하드웨어를 더 잘 제어할 수 있고 FPGA 리소스를 최적으로 활용할 수 있습니다.

##### 2. 
따라서 일반적인 FINN 하드웨어 빌딩 블록의 RTL 변형이 점점 더 많아짐에 따라 'step_specialize_layers'라는 추가 빌더 단계를 도입했습니다. 

이 단계에서는 HW 노드가 HLS 또는 RTL 변형 노드로 특화됩니다. 

##### 3. 
미리 결정된 규칙에 따라 변환되거나 사용자가 원하는 설정이 포함된 구성 파일을 제공하면 변환됩니다. 

사용자 기본 설정을 충족할 수 없는 경우 경고가 표시되고 구현 스타일이 기본값으로 설정됩니다. 

##### 4. 
step_create_dataflow_partition` 이전의 빌더 흐름 단계에서는 레이어별로 선호하는 구현 스타일을 설정하기 위한 템플릿 json 파일을 생성합니다. 

이전 실행 중 하나에서 이 폴더로 복사하여 새 빌드에 전달하도록 조작할 수 있습니다.

```python
import json

with open(build_dir+"/output_pre_and_post_proc/template_specialize_layers_config.json", 'r') as json_file:
    specialize_layers_config = json.load(json_file)

print(json.dumps(specialize_layers_config, indent=1))
```
```bash
#출력
{
 "Defaults": {},
 "Thresholding_0": {
  "preferred_impl_style": ""
 },
 "ConvolutionInputGenerator_0": {
  "preferred_impl_style": ""
 },
 "MVAU_0": {
  "preferred_impl_style": ""
 },
 "ConvolutionInputGenerator_1": {
  "preferred_impl_style": ""
 },
 "MVAU_1": {
  "preferred_impl_style": ""
 },
 "StreamingMaxPool_0": {
  "preferred_impl_style": ""
 },
#길어서 중간 생략
```

보시다시피 각 노드가 .json 파일에 나열되어 있으며 노드 속성 `preferred_impl_style`에 대한 빈 문자열이 기본적으로 인스턴스화되어 있습니다.

이제 이 .json을 사용하여 새 빌더 플로우에 전달할 `preferred_impl_style`을 설정할 수 있습니다.

```python
with open(build_dir+"/output_pre_and_post_proc/template_specialize_layers_config.json", 'r') as json_file:
    specialize_layers_config = json.load(json_file)

# Set all preferred_impl_style to all HLS
for key in specialize_layers_config:
    if "preferred_impl_style" in specialize_layers_config[key]:
        specialize_layers_config[key]["preferred_impl_style"] = "hls" 
# Save as .json    
with open("specialize_layers_all_hls.json", "w") as jsonFile:
    json.dump(specialize_layers_config, jsonFile)
         
# Set SWG to RTL variant
for key in specialize_layers_config:
    if "preferred_impl_style" in specialize_layers_config[key]:
        if key.startswith("ConvolutionInputGenerator"):
            specialize_layers_config[key]["preferred_impl_style"] = "rtl"
        else:
            specialize_layers_config[key]["preferred_impl_style"] = "hls"  
# Save as .json    
with open("specialize_layers_swg_rtl.json", "w") as jsonFile:
    json.dump(specialize_layers_config, jsonFile)
```
<br>

두 개의 `specialize_layers_config_파일`을 생성했습니다:
* 하나는 모든 레이어를 `“hls”`로 설정합니다.
* 컨볼루션 입력 생성기에 대한 `preferred_impl_style`을 `“rtl”`로 설정하는 파일입니다.

#### 8.10. 이해하기
바로 위 코드는 FINN 프레임워크에서 레이어 구현 스타일을 설정하는 두 가지 다른 구성 파일을 생성하고 최적화하는 것이라고 이해하였습니다.

1. 기존의 `specialize_layers_config.json` 파일을 열고 JSON 형식으로 로드합니다.

2. 모든 레이어의 `preferred_impl_sytle` 을 `hls` 로 설정합니다.

3. 수정된 설정을 `specialize_layers_all_hls.json` 파일로 저장합니다.

4. `ConvolutionInputGenerator` 레이어의 `preferred_impl_style` 을 `rtl`로 설정하고, 나머지는 `hls`로 설정합니다.

5. 수정된 설정을 `specialize_layers_swg_rtl.json` 파일로 저장합니다.
    - 이 과정을 통해 두 가지 다른 구현 전략을 가진 설정 파일이 생성됩니다:
        - 모든 레이어를 HLS(High-Level Synthesis)로 구현
        - ConvolutionInputGenerator를 RTL(Register-Transfer Level)로, 나머지를 HLS로 구현

---
## 8.11. Specialize layers configuration json - 2
---
다음에서는 두 개의 빌드 흐름을 설정하고 예상 보고서 단계로 실행해 보겠습니다.

그런 다음 중간 .onnx 파일을 조사하고 두 실행을 비교하겠습니다.
```python
## Build flow with custom folding configuration
## specialize_layers_config_file = "specialize_layers_all_hls.json"

model_dir = os.environ['FINN_ROOT'] + "/notebooks/advanced"
model_file = model_dir + "/end2end_cnv_w2a2_export.onnx"

output_dir = build_dir + "/output_all_hls"

#Delete previous run results if exist
if os.path.exists(output_dir):
    shutil.rmtree(output_dir)
    print("Previous run results deleted!")

build_steps = [
    custom_step_add_pre_proc,
    custom_step_add_post_proc,
    "step_qonnx_to_finn",
    "step_tidy_up",
    "step_streamline",
    "step_convert_to_hw",
    "step_create_dataflow_partition",
    "step_specialize_layers",
    "step_apply_folding_config",
    "step_minimize_bit_width",
    "step_generate_estimate_reports",
]

cfg_estimates = build.DataflowBuildConfig(
    output_dir          = output_dir,
    mvau_wwidth_max     = 80,
    synth_clk_period_ns = 10.0,
    fpga_part           = "xc7z020clg400-1",
    steps               = build_steps,
    specialize_layers_config_file = "specialize_layers_all_hls.json",
    generate_outputs=[
        build_cfg.DataflowOutputType.ESTIMATE_REPORTS,
    ]
)
```
<br>

```python
%%time
build.build_dataflow_cfg(model_file, cfg_estimates);
```
```bash
#출력
Building dataflow accelerator from /home/lsh/FPGA/finn/notebooks/advanced/end2end_cnv_w2a2_export.onnx
Intermediate outputs will be generated in /tmp/finn_dev_lsh
Final outputs will be generated in /home/lsh/FPGA/finn/notebooks/advanced/output_all_hls
Build log is at /home/lsh/FPGA/finn/notebooks/advanced/output_all_hls/build_dataflow.log
Running step: custom_step_add_pre_proc [1/11]
Running step: custom_step_add_post_proc [2/11]
Running step: step_qonnx_to_finn [3/11]
Running step: step_tidy_up [4/11]
Running step: step_streamline [5/11]
Running step: step_convert_to_hw [6/11]
Running step: step_create_dataflow_partition [7/11]
Running step: step_specialize_layers [8/11]
Running step: step_apply_folding_config [9/11]
Running step: step_minimize_bit_width [10/11]
Running step: step_generate_estimate_reports [11/11]
Completed successfully
CPU times: user 3.67 s, sys: 76.2 ms, total: 3.74 s
Wall time: 3.74 s
```
<br>

```python
## Build flow with custom folding configuration
## specialize_layers_config_file = "specialize_layers_swg_rtl.json"

model_dir = os.environ['FINN_ROOT'] + "/notebooks/advanced"
model_file = model_dir + "/end2end_cnv_w2a2_export.onnx"

output_dir = build_dir + "/output_swg_rtl"

#Delete previous run results if exist
if os.path.exists(output_dir):
    shutil.rmtree(output_dir)
    print("Previous run results deleted!")

build_steps = [
    custom_step_add_pre_proc,
    custom_step_add_post_proc,
    "step_qonnx_to_finn",
    "step_tidy_up",
    "step_streamline",
    "step_convert_to_hw",
    "step_create_dataflow_partition",
    "step_specialize_layers",
    "step_apply_folding_config",
    "step_minimize_bit_width",
    "step_generate_estimate_reports",
]

cfg_estimates = build.DataflowBuildConfig(
    output_dir          = output_dir,
    mvau_wwidth_max     = 80,
    synth_clk_period_ns = 10.0,
    fpga_part           = "xc7z020clg400-1",
    steps               = build_steps,
    specialize_layers_config_file = "specialize_layers_swg_rtl.json",
    generate_outputs=[
        build_cfg.DataflowOutputType.ESTIMATE_REPORTS,
    ]
)
```
<br>

```python
%%time
build.build_dataflow_cfg(model_file, cfg_estimates);
```
```bash
#출력
Building dataflow accelerator from /home/lsh/FPGA/finn/notebooks/advanced/end2end_cnv_w2a2_export.onnx
Intermediate outputs will be generated in /tmp/finn_dev_lsh
Final outputs will be generated in /home/lsh/FPGA/finn/notebooks/advanced/output_swg_rtl
Build log is at /home/lsh/FPGA/finn/notebooks/advanced/output_swg_rtl/build_dataflow.log
Running step: custom_step_add_pre_proc [1/11]
Running step: custom_step_add_post_proc [2/11]
Running step: step_qonnx_to_finn [3/11]
Running step: step_tidy_up [4/11]
Running step: step_streamline [5/11]
Running step: step_convert_to_hw [6/11]
Running step: step_create_dataflow_partition [7/11]
Running step: step_specialize_layers [8/11]
Running step: step_apply_folding_config [9/11]
Running step: step_minimize_bit_width [10/11]
Running step: step_generate_estimate_reports [11/11]
Completed successfully
CPU times: user 3.72 s, sys: 47.7 ms, total: 3.77 s
Wall time: 3.77 s
```
<br>

#### 8.11. 이해하기
`8.10.1. Specialize layers configuration json - 1` 단계에서 생성된 2개의 `.json` 파일을 각각 적용시켜 2개의의 빌드를 생성합니다.

---
## 8.12. Specialize layers configuration json - 3
---
먼저 'step_create_dataflow_partition' 다음에 'step_specialize_layers' 다음에 중간 모델을 살펴봅니다.

```python
showInNetron(build_dir+"/output_all_hls/intermediate_models/step_create_dataflow_partition.onnx")
```
<br>
-오닉스 출력

먼저 '모든 HLS'에 특화된 모델을 살펴보겠습니다.

```python
showInNetron(build_dir+"/output_all_hls/intermediate_models/step_specialize_layers.onnx")
```
<br>
-오닉스출력

보시다시피, 이제 각 작업 유형에는 해당 노드의 HLS 변형임을 나타내는 접미사가 있습니다.

또한 Netron 시각화에서 노드 중 하나를 클릭하면 해당 모듈이 `finn.custom_op.fpgadataflow.hls`로 설정되어 있는 것을 볼 수 있습니다.

이제 컨볼루션 입력 생성기를 `“rtl”`로 특화시킨 모델을 살펴보겠습니다.

```python
showInNetron(build_dir+"/output_swg_rtl/intermediate_models/step_specialize_layers.onnx")
```
<br>

-오닉스 출력

위의 셀을 사용하여 다양한 설정을 시도하고 빌더 플로우에 전달할 수 있습니다.

모든 레이어에 HLS 및 RTL 변형이 있는 것은 아니므로 `specialize_layers_config.json`에서 정의한 설정이 무시되고 대신 합리적인 기본값이 설정될 수 있습니다.

이 경우 FINN 컴파일러는 경고를 표시합니다.

#### 8.12. 이해하기


---
## 8.13. Folding configuration json
---
**폴딩 구성 Json**
##### 1. 
FINN 컴파일러를 사용하면 스트리밍 데이터 흐름 아키텍처에서 네트워크를 구현할 수 있으며, 이는 모든 계층이 개별적으로 구현되고 데이터가 가속기를 통해 스트리밍된다는 것을 의미합니다.

각 계층의 병렬 처리와 리소스 유형을 조정하여 특정 성능 및 리소스 요구 사항에 맞게 각 계층을 사용자 정의할 수 있습니다.

FINN에서는 각 레이어에서 이러한 병렬 처리 사용자 지정을 폴딩이라고 합니다.

FINN에서 폴딩 인자/병렬 처리의 영향에 대해 자세히 알아보려면 [폴딩 튜토리얼](./3_folding.ipynb)을 참조하세요.

이 섹션에서는 FINN 빌더 도구를 사용하여 각 레이어의 커스터마이징에 영향을 줄 수 있는 인터페이스를 살펴보겠습니다: 

접기 구성이 포함된 json 파일입니다.

##### 2. 
호출된 단계에 따라 FINN 컴파일러는 각 레이어에 대한 폴딩 구성이 포함된 .json 파일을 생성하거나 사용할 수 있습니다.

아래 셀에서는 `step_target_fps_parallelization`에 의해 자동으로 생성된 .json 파일을 살펴보겠습니다.

그런 다음 이 파일을 시작점으로 사용하여 폴딩 구성을 조작하고 빌더 툴에 다시 공급합니다.

```python
import json

with open(build_dir+"/output_pre_and_post_proc/auto_folding_config.json", 'r') as json_file:
    folding_config = json.load(json_file)

print(json.dumps(folding_config, indent=1))
```
```bash
#출력
{
 "Defaults": {},
 "Thresholding_rtl_0": {
  "PE": 1,
  "runtime_writeable_weights": 0,
  "depth_trigger_uram": 0,
  "depth_trigger_bram": 0
 },
 "ConvolutionInputGenerator_rtl_0": {
  "SIMD": 3,
  "parallel_window": 0,
  "ram_style": "distributed"
 },
 "MVAU_hls_0": {
  "PE": 8,
  "SIMD": 27,
  "ram_style": "auto",
  "resType": "auto",
  "mem_mode": "internal_decoupled",
  "runtime_writeable_weights": 0
 },
 # 코드가 길어서 생략
```
<br>

위의 인쇄된 셀에서 볼 수 있듯이 .json 파일의 키는 네트워크에 있는 레이어의 노드 이름입니다.

각 레이어마다 몇 가지 노드 속성이 나열되어 있습니다:
- `PE`와 `SIMD`는 각 레이어의 병렬성을 결정하는 폴딩 파라미터로, 레이어에 따라 다른 값으로 설정할 수 있으며, 자세한 내용은 [이 표](https://finn-dev.readthedocs.io/en/latest/internals.html#constraints-to-folding-factors-per-layer)를 참조하세요.

- mem_mode`: 파라미터 메모리를 HLS/RTL 코드의 일부로 구현할지(`const`), 아니면 별도로 인스턴스화하여 메모리 스트리머 유닛을 통해 레이어와 연결할지(`decoupled`) 결정합니다.자세한 내용은 이 문서에서 확인할 수 있습니다: https://finn-dev.readthedocs.io/en/latest/internals.html#matrixvectoractivation-mem-mode . 외부 가중치를 구현할 수 있도록 mem_mode를 외부로 설정하는 것도 가능합니다.

- `ram_style`: `decoupled` 모드를 선택할 때, FINN 컴파일러는 레이어에 사용할 메모리 리소스를 선택할 수 있습니다. 인자 `ram_style`은 선택한 메모리 유형으로 설정됩니다:
    - `auto`: 구현이 LUTRAM 또는 BRAM을 사용하는지 여부는 Vivado가 결정합니다.

    - `distributed`: LUTRAM이 사용됩니다.

    - `block`: BRAM이 사용됩니다.

    - `ultra`: 선택한 보드에서 사용 가능한 경우, URAM이 사용됩니다.

- `resType`: MVAU 레이어의 노드 속성이며 `lut` 또는 `dsp`로 설정할 수 있습니다. dsp`를 선택하면 MVAU의 최적화된 RTL 변형이 활성화되지 않고 DSP를 활용한 HLS 코드가 생성되므로 아직 최적은 아니지만 설계 공간 탐색을 위한 추가 파라미터를 제공할 수 있습니다.

- `runtime_writeable_weights`: FINN은 가중치를 “런타임 쓰기 가능”으로 구현하는 옵션을 제공합니다. 즉, 액실라이트 인터페이스를 통해 드라이버에서 가중치 값을 쓸 수 있습니다.


튜토리얼의 다음 부분에서는 자동 생성된 json 파일을 시작점으로 사용하여 `ram_style` 속성을 탐색하는 두 개의 새 json 파일을 만들겠습니다. 

FINN 빌더에서 생성된 보고서 중 하나를 사용하여 이러한 변경 사항의 영향을 확인하겠습니다.

이를 위해 다음 셀에 있는 *estimate_layer_resources.json* 보고서에서 총 리소스를 추출합니다.

```python
with open(build_dir+"/output_pre_and_post_proc/report/estimate_layer_resources.json", 'r') as json_file:
    json_object = json.load(json_file)

print(json.dumps(json_object["total"], indent=1))
```
```bash
{
 "BRAM_18K": 485.0,
 "LUT": 99867.0,
 "URAM": 0.0,
 "DSP": 0.0
}
```
<br>
FINN 컴파일러는 네트워크가 최대 500개의 BRAM 블록과 최대 100,000개의 LUT를 사용할 것으로 추정합니다.

자동 폴딩 설정 파일`을 사용하여 해당 파일에서 두 개의 폴딩 설정을 생성합니다:
- 모든 `ram_style` 속성을 `distributed`로 설정합니다.
- 모든 `ram_style` 속성이 `block`으로 설정합니다.

```python
with open(build_dir+"/output_pre_and_post_proc/auto_folding_config.json", 'r') as json_file:
    folding_config = json.load(json_file)

# Set all ram_style to LUT RAM
for key in folding_config:
    if "ram_style" in folding_config[key]:
        folding_config[key]["ram_style"] = "distributed" 
# Save as .json    
with open("folding_config_all_lutram.json", "w") as jsonFile:
    json.dump(folding_config, jsonFile)
         
# Set all ram_style to BRAM
for key in folding_config:
    if "ram_style" in folding_config[key]:
        folding_config[key]["ram_style"] = "block" 
# Save as .json    
with open("folding_config_all_bram.json", "w") as jsonFile:
    json.dump(folding_config, jsonFile)
```
<br>

이러한 파일을 생성한 후 빌더 플로우를 호출합니다.

FINN 빌더가 생성된 폴딩 구성을 입력으로 받도록 하려면 추가 빌더 인수 `folding_config_file`을 설정해야 하며 `build_steps`를 `step_target_fps_parallelization`을 실행하지 않도록 변경할 것입니다.

빌드 단계를 반드시 제외할 필요는 없지만, 별도의 폴딩 구성을 전달하기 때문에 해당 단계의 출력을 덮어쓰게 되므로 빠른 실행을 위해 건너뜁니다.

```python
## Build flow with custom folding configuration
## folding_config_file = "folding_config_all_lutram.json"

model_dir = os.environ['FINN_ROOT'] + "/notebooks/advanced"
model_file = model_dir + "/end2end_cnv_w2a2_export.onnx"

output_dir = build_dir + "/output_all_lutram"

#Delete previous run results if exist
if os.path.exists(output_dir):
    shutil.rmtree(output_dir)
    print("Previous run results deleted!")

build_steps = [
    custom_step_add_pre_proc,
    custom_step_add_post_proc,
    "step_qonnx_to_finn",
    "step_tidy_up",
    "step_streamline",
    "step_convert_to_hw",
    "step_create_dataflow_partition",
    "step_specialize_layers",
    "step_apply_folding_config",
    "step_minimize_bit_width",
    "step_generate_estimate_reports",
]

cfg_estimates = build.DataflowBuildConfig(
    output_dir          = output_dir,
    mvau_wwidth_max     = 80,
    synth_clk_period_ns = 10.0,
    fpga_part           = "xc7z020clg400-1",
    steps               = build_steps,
    folding_config_file = "folding_config_all_lutram.json",
    generate_outputs=[
        build_cfg.DataflowOutputType.ESTIMATE_REPORTS,
    ]
)
```
<br>

```python
%%time
build.build_dataflow_cfg(model_file, cfg_estimates);
```
```bash
#출력
Building dataflow accelerator from /home/lsh/FPGA/finn/notebooks/advanced/end2end_cnv_w2a2_export.onnx
Intermediate outputs will be generated in /tmp/finn_dev_lsh
Final outputs will be generated in /home/lsh/FPGA/finn/notebooks/advanced/output_all_lutram
Build log is at /home/lsh/FPGA/finn/notebooks/advanced/output_all_lutram/build_dataflow.log
Running step: custom_step_add_pre_proc [1/11]
Running step: custom_step_add_post_proc [2/11]
Running step: step_qonnx_to_finn [3/11]
Running step: step_tidy_up [4/11]
Running step: step_streamline [5/11]
Running step: step_convert_to_hw [6/11]
Running step: step_create_dataflow_partition [7/11]
Running step: step_specialize_layers [8/11]
Running step: step_apply_folding_config [9/11]
Running step: step_minimize_bit_width [10/11]
Running step: step_generate_estimate_reports [11/11]
Completed successfully
CPU times: user 3.64 s, sys: 66.3 ms, total: 3.71 s
Wall time: 3.71 s
```
<br>

이제 생성된 모델을 살펴보면, 개별 노드를 클릭하면 모든 레이어의 노드 속성 'ram_style'이 'distributed'으로 설정되어 있는 것을 확인할 수 있습니다.

```python
showInNetron(build_dir+"/output_all_lutram/intermediate_models/step_generate_estimate_reports.onnx")
```
<br>

-오닉스 팡리

```python
with open(build_dir+"/output_all_lutram/report/estimate_layer_resources.json", 'r') as json_file:
    json_object = json.load(json_file)

print(json.dumps(json_object["total"], indent=1))
```
```bash
with open(build_dir+"/output_all_lutram/report/estimate_layer_resources.json", 'r') as json_file:
    json_object = json.load(json_file)

print(json.dumps(json_object["total"], indent=1))
```
<br>

추정 보고서에 따르면 BRAM 사용률은 0으로 떨어지고 LUT 수는 약 15만 개로 증가했습니다.

모든 메모리 리소스가 BRAM을 사용하도록 설정하는 폴딩 구성도 똑같이 해보겠습니다

```python
## Build flow with custom folding configuration
## folding_config_file = "folding_config_all_bram.json"

model_dir = os.environ['FINN_ROOT'] + "/notebooks/advanced"
model_file = model_dir + "/end2end_cnv_w2a2_export.onnx"

output_dir = build_dir + "/output_all_bram"

#Delete previous run results if exist
if os.path.exists(output_dir):
    shutil.rmtree(output_dir)
    print("Previous run results deleted!")

build_steps = [
    custom_step_add_pre_proc,
    custom_step_add_post_proc,
    "step_qonnx_to_finn",
    "step_tidy_up",
    "step_streamline",
    "step_convert_to_hw",
    "step_create_dataflow_partition",
    "step_specialize_layers",
    "step_apply_folding_config",
    "step_minimize_bit_width",
    "step_generate_estimate_reports",
]

cfg_estimates = build.DataflowBuildConfig(
    output_dir          = output_dir,
    mvau_wwidth_max     = 80,
    synth_clk_period_ns = 10.0,
    fpga_part           = "xc7z020clg400-1",
    steps               = build_steps,
    folding_config_file = "folding_config_all_bram.json",
    generate_outputs=[
        build_cfg.DataflowOutputType.ESTIMATE_REPORTS,
    ]
)
```
<br>

```python
%%time
build.build_dataflow_cfg(model_file, cfg_estimates);
```
```bash
#출력
Building dataflow accelerator from /home/lsh/FPGA/finn/notebooks/advanced/end2end_cnv_w2a2_export.onnx
Intermediate outputs will be generated in /tmp/finn_dev_lsh
Final outputs will be generated in /home/lsh/FPGA/finn/notebooks/advanced/output_all_bram
Build log is at /home/lsh/FPGA/finn/notebooks/advanced/output_all_bram/build_dataflow.log
Running step: custom_step_add_pre_proc [1/11]
Running step: custom_step_add_post_proc [2/11]
Running step: step_qonnx_to_finn [3/11]
Running step: step_tidy_up [4/11]
Running step: step_streamline [5/11]
Running step: step_convert_to_hw [6/11]
Running step: step_create_dataflow_partition [7/11]
Running step: step_specialize_layers [8/11]
Running step: step_apply_folding_config [9/11]
Running step: step_minimize_bit_width [10/11]
Running step: step_generate_estimate_reports [11/11]
Completed successfully
CPU times: user 3.7 s, sys: 177 ms, total: 3.87 s
Wall time: 3.87 s
```
<br>

```python
showInNetron(build_dir+"/output_all_bram/intermediate_models/step_generate_estimate_reports.onnx")
```
<br>

- 오닉스 파일

```python
with open(build_dir+"/output_all_bram/report/estimate_layer_resources.json", 'r') as json_file:
    json_object = json.load(json_file)

print(json.dumps(json_object["total"], indent=1))
```
```bash
{
 "BRAM_18K": 494.0,
 "LUT": 99049.0,
 "URAM": 0.0,
 "DSP": 0.0
}
```
<br>

초기 구현에서는 이미 BRAM의 활용도가 높았지만, 현재는 최대 500개의 BRAM으로 추정되는 반면 LUT 수는 최대 9만 9천 개로 감소했습니다.

이 예제를 시작점으로 삼아 폴딩 구성을 직접 조작할 수 있습니다.

위의 코드를 사용하는 대신 예제 .json 파일 중 하나를 수동으로 열고 값을 다르게 설정할 수도 있습니다.

노드 속성은 임의의 값으로 설정할 수 없다는 점에 유의하세요.

특히 접는 요소는 [특정 제약 조건]을 충족해야 합니다(https://finn-dev.readthedocs.io/en/latest/internals.html#constraints-to-folding-factors-per-layer).

노드 속성에 대한 다른 설정은 개별 커스텀 연산자 클래스에서 가장 잘 찾아볼 수 있습니다: [예를 들어 MVAU의 경우](https://github.com/Xilinx/finn/blob/dev/src/finn/custom_op/fpgadataflow/matrixvectoractivation.py#L64)

#### 8.13. 이해하기
`8.12`에서 진행한 것처럼 값을 변경하여 실제 사용되는 부분을 변경할 수 있습니다.

---
## 8.14. Additional builder arguments - 1
---
**추가 빌더 인자**  
##### 1. 
이 섹션에서는 FINN 컴파일러가 노출하는 추가 빌더 인수를 집중적으로 살펴봅니다.

모든 것을 다룰 수는 없지만 목록을 살펴볼 수 있으며, 시간을 내어 FINN 빌더 구성을 사용자 지정할 수 있는 다양한 옵션을 살펴보는 것이 좋습니다.

##### 2. 
먼저 빌더에서 검증 흐름을 활성화하는 것으로 시작합니다. 

FINN 컴파일러는 모델이 하드웨어로 전환되기 전에 여러 가지 변환을 적용하므로 네트워크의 기능적 동작이 변경되지 않는지 확인해야 합니다.

**검증 단계**

튜토리얼의 앞부분에서 빌드 단계가 어떻게 작성되는지 살펴봤습니다. 

`step_tidy_up`을 살펴보면 변경된 모델을 반환하기 전에 검증 단계를 실행할 수 있음을 알 수 있습니다.

`step_tidy_up`의 경우 `VerificationStepType.TIDY_UP_PYTHON`을 설정하여 시작할 수 있는 `"initial python”` 단계입니다.

```python
import finn.builder.build_dataflow_steps as build_dataflow_steps
showSrc(build_dataflow_steps.step_tidy_up)
```
```bash
#출력
def step_tidy_up(model: ModelWrapper, cfg: DataflowBuildConfig):
    """Run the tidy-up step on given model. This includes shape and datatype
    inference, constant folding, and giving nodes and tensors better names.
    """

    model = model.transform(InferShapes())
    model = model.transform(FoldConstants())
    model = model.transform(GiveUniqueNodeNames())
    model = model.transform(GiveReadableTensorNames())
    model = model.transform(InferDataTypes())
    model = model.transform(RemoveStaticGraphInputs())

    if VerificationStepType.TIDY_UP_PYTHON in cfg._resolve_verification_steps():
        verify_step(model, cfg, "initial_python", need_parent=False)

    return model
```
<br>

기본 빌드 단계 중 일부는 해당 확인 단계가 설정되어 있는 경우 자동 확인이 활성화되어 있습니다.

```python
showSrc(build_cfg.VerificationStepType)
```
```bash
#출력
class VerificationStepType(str, Enum):
    "Steps at which FINN ONNX execution can be launched for verification."

    #: verify after step_qonnx_to_finn, using Python execution
    QONNX_TO_FINN_PYTHON = "finn_onnx_python"
    #: verify after step_tidy_up, using Python execution
    TIDY_UP_PYTHON = "initial_python"
    #: verify after step_streamline , using Python execution
    STREAMLINED_PYTHON = "streamlined_python"
    #: verify after step_apply_folding_config, using C++ for each HLS node
    FOLDED_HLS_CPPSIM = "folded_hls_cppsim"
    #: verify after step_create_stitched_ip, using stitched-ip Verilog
    STITCHED_IP_RTLSIM = "stitched_ip_rtlsim"
```
<br>

아래 셀에서는 CIFAR-10 데이터 세트의 예제 입력을 사용하고 브레비타스의 포워드 패스를 사용하여 참조 출력을 생성합니다.

입력은 `input.npy`로 저장하고 참조 출력은 `expected_output.npy`로 저장합니다.

```python
# Get golden io pair from Brevitas and save as .npy files
from finn.util.test import get_trained_network_and_ishape, get_example_input, get_topk
import numpy as np


(brevitas_model, ishape) = get_trained_network_and_ishape("cnv", 2, 2)
input_tensor_npy = get_example_input("cnv")
input_tensor_torch = torch.from_numpy(input_tensor_npy).float()
input_tensor_torch = ToTensor().forward(input_tensor_torch).detach()
output_tensor_npy = brevitas_model.forward(input_tensor_torch).detach().numpy()
output_tensor_npy = get_topk(output_tensor_npy, k=1)

np.save("input.npy", input_tensor_npy)
np.save("expected_output.npy", output_tensor_npy)
```

#### 8.14. 이해하기
1.  `step` 단계에는 검증하는 부분이 있는데 이는 `build_cfg.VerificationStepType` 으로 확인할 수 있습니다.
    - 총 5개 스탭에 해당하는 부분에서 검증 부분이 있습니다.

2. 이를 참조 및 출력하기 위해 모듈을 임포트 합니다.
    - `finn.util.test` 와 `numpy` 를 임포트하여 검증을 위해 네트워크의 참조 입력과 출력을 생성합니다.

    - `brevitas_model` 을 불러옵니다.

    - 입력 데이터를 생성합니다.

    - `ToTensor` 를 사용합니다.

    - 참초 출력을 생성합니다.
        - `forward` 및 `topk` 가 사용됩니다.

    - 데이터를 저장합니다.


---
## 8.15. Additional builder arguments - 2
---
다음 단계에서는 빌더 플로우를 다시 설정하는데, 이번에는 빌드 인자 `verify_steps`를 설정하고 확인 단계 목록을 전달합니다.
```python
## Build flow with additional builder arguments enabled
## verification steps

model_dir = os.environ['FINN_ROOT'] + "/notebooks/advanced"
model_file = model_dir + "/end2end_cnv_w2a2_export.onnx"

output_dir = build_dir + "/output_with_verification"

#Delete previous run results if exist
if os.path.exists(output_dir):
    shutil.rmtree(output_dir)
    print("Previous run results deleted!")

build_steps = [
    custom_step_add_pre_proc,
    custom_step_add_post_proc,
    "step_qonnx_to_finn",
    "step_tidy_up",
    "step_streamline",
    "step_convert_to_hw",
    "step_create_dataflow_partition",
    "step_specialize_layers",
    "step_target_fps_parallelization",
    "step_apply_folding_config",
    "step_minimize_bit_width",
    "step_generate_estimate_reports",
]

cfg_estimates = build.DataflowBuildConfig(
    output_dir          = output_dir,
    mvau_wwidth_max     = 80,
    target_fps          = 10000,
    synth_clk_period_ns = 10.0,
    fpga_part           = "xc7z020clg400-1",
    steps               = build_steps,
    generate_outputs=[
        build_cfg.DataflowOutputType.ESTIMATE_REPORTS,
    ],
    verify_steps=[
        build_cfg.VerificationStepType.QONNX_TO_FINN_PYTHON,
        build_cfg.VerificationStepType.TIDY_UP_PYTHON,
        build_cfg.VerificationStepType.STREAMLINED_PYTHON,
    ]
)
```
<br>

아래 코드를 실행하면 백그라운드에서 검증이 호출됩니다. 

실행 후 출력 디렉토리를 조사하여 검증이 성공했는지 확인할 수 있습니다.

```python
%%time
build.build_dataflow_cfg(model_file, cfg_estimates);
```
```bash
Building dataflow accelerator from /home/lsh/FPGA/finn/notebooks/advanced/end2end_cnv_w2a2_export.onnx
Intermediate outputs will be generated in /tmp/finn_dev_lsh
Final outputs will be generated in /home/lsh/FPGA/finn/notebooks/advanced/output_with_verification
Build log is at /home/lsh/FPGA/finn/notebooks/advanced/output_with_verification/build_dataflow.log
Running step: custom_step_add_pre_proc [1/12]
Running step: custom_step_add_post_proc [2/12]
Running step: step_qonnx_to_finn [3/12]
Running step: step_tidy_up [4/12]
Running step: step_streamline [5/12]
Running step: step_convert_to_hw [6/12]
Running step: step_create_dataflow_partition [7/12]
Running step: step_specialize_layers [8/12]
Running step: step_target_fps_parallelization [9/12]
Running step: step_apply_folding_config [10/12]
Running step: step_minimize_bit_width [11/12]
Running step: step_generate_estimate_reports [12/12]
Completed successfully
CPU times: user 19.2 s, sys: 283 ms, total: 19.4 s
Wall time: 19 s
```
<br>

이제 출력 디렉터리에 `verification_output`이라는 추가 디렉터리가 생겼습니다.

```python
!ls {build_dir}/output_with_verification
```
```bash
auto_folding_config.json  template_specialize_layers_config.json
intermediate_models	  time_per_step.json
report			  verification_output
```
<br>

```python
!ls {build_dir}/output_with_verification/verification_output
```
```bash
verify_initial_python_0_SUCCESS.npy
verify_qonnx_to_finn_python_0_SUCCESS.npy
verify_streamlined_python_0_SUCCESS.npy
```
<br>

이 디렉터리에는 3개의 .npy 파일이 있습니다. 이 파일은 여러 검증 단계에서 저장된 출력 파일입니다.

접미사는 배열이 예상 출력과 일치하는지 여부를 나타냅니다. 이 경우 접미사는 모든 검증 단계 `_SUCCESS`에 대한 것입니다

출력은 .npy로 저장되므로 Python에서 간단히 파일을 열고 조사할 수 있습니다.

```python
verify_initial_python = np.load(build_dir + "/output_with_verification/verification_output/verify_initial_python_0_SUCCESS.npy")
print("The output of the verification step after the step_tidy_up is: " + str(verify_initial_python))
```
```bash
#출력
The output of the verification step after the step_tidy_up is: [3]
```
<br>

생성된 출력이 예상 출력과 일치하지 않는 경우 이러한 파일을 디버깅에 사용할 수 있습니다.

### 8.15.2. 이해하기

검증 과정을 위해 `build_step` 에 `verify_steps=` 이 포함됩니다.

---
### 8.16.1. Additional builder arguments - 3
##### 기타 빌드 인수 - 1
---

 
인증 플로우 활성화 외에도 FINN 빌더에는 네트워크를 더욱 맞춤 설정할 수 있는 다양한 추가 빌더 인수가 있습니다. 

인수의 옵션을 살펴보겠습니다. 

여기서는 FINN 관련 인자만 필터링하겠습니다.

```python
# Filter out methods
builder_args = [m for m in dir(build_cfg.DataflowBuildConfig) if not m.startswith('_')]
print("\n".join(builder_args))
```
```bash
#출력
auto_fifo_depths
auto_fifo_strategy
board
default_swg_exception
enable_build_pdb_debug
enable_hw_debug
folding_config_file
folding_two_pass_relaxation
force_python_rtlsim
fpga_part
from_dict
from_json
hls_clk_period_ns
large_fifo_mem_style
max_multithreshold_bit_width
minimize_bit_width
mvau_wwidth_max
rtlsim_batch_size
rtlsim_use_vivado_comps
save_intermediate_models
schema
shell_flow_type
signature
specialize_layers_config_file
split_large_fifos
standalone_thresholds
start_step
steps
stitched_ip_gen_dcp
stop_step
target_fps
to_dict
to_json
verbose
verify_expected_output_npy
verify_input_npy
verify_save_full_context
verify_save_rtlsim_waveforms
verify_steps
vitis_floorplan_file
vitis_opt_strategy
vitis_platform
```
<br>

데이터 클래스-json 클래스에서 오는 어트리뷰트가 있습니다: `to_dict`, `to_json`, `schema`, `from_json`, `from_dict`. 

이 클래스는 FINN 빌더의 구현에 사용됩니다. 

이 튜토리얼에서는 주로 FINN 관련 인자에 관심이 있습니다.  

이러한 인수 중 일부는 사이버 보안 노트북과 이 노트북에서 이미 살펴본 바 있습니다(예: `target_fps`, `fpga_part` 및 `folding_config_file`). 

FINN 빌더의 코드에서 각 빌더 인수의 기능은 문서로 되어 있으며, [여기](https://github.com/Xilinx/finn/blob/dev/src/finn/builder/build_dataflow_config.py#L155)에서 사용 가능한 빌더 인수를 스크롤하여 확인할 수 있으며, 검증 흐름 활성화 외에도 FINN 빌더에는 네트워크를 추가로 사용자 정의할 수 있는 수많은 추가 빌더 인수가 있습니다. 

인수의 옵션을 살펴보겠습니다. 

여기서는 FINN 관련 인자만 필터링하겠습니다.


#### 8.16. 이해하기

FINN 빌더의 DataflowBuildConfig 클래스에는 네트워크 가속기를 더욱 세밀하게 맞춤 설정할 수 있는 다양한 추가 인수들이 있습니다. 

주요 인수들은 다음과 같습니다:
1. 성능 최적화 관련:
    - target_fps: 목표 프레임 레이트 설정
    - mvau_wwidth_max: MVAU(Matrix-Vector Activation Unit)의 최대 너비 설정
    - folding_config_file: 폴딩 구성 파일 지정
    - auto_fifo_depths: FIFO 깊이 자동 설정
    - auto_fifo_strategy: FIFO 전략 자동 설정

2. 하드웨어 구현 관련:
    - fpga_part: 대상 FPGA 부품 지정
    
    - hls_clk_period_ns: HLS 클럭 주기 설정

    - board: 대상 보드 지정

    - vitis_platform: Vitis 플랫폼 지정

    - vitis_floorplan_file: Vitis 플로어플랜 파일 지정
 
3. 최적화 및 구현 전략:
    - specialize_layers_config_file: 레이어 특수화 구성 파일 지정

    - minimize_bit_width: 비트 폭 최소화 활성화

    - vitis_opt_strategy: Vitis 최적화 전략 설정
4. 시뮬레이션 및 검증:
    - rtlsim_batch_size: RTL 시뮬레이션 배치 크기 설정

    - verify_steps: 검증 단계 지정

    - verify_input_npy: 검증용 입력 파일 지정

    - verify_expected_output_npy: 예상 출력 파일 지정

5. 디버깅 및 중간 결과:
    - save_intermediate_models: 중간 모델 저장 활성화

    - enable_hw_debug: 하드웨어 디버깅 활성화

    - enable_build_pdb_debug: 빌드 PDB 디버깅 활성화

6. 기타 설정:
    - steps: 실행할 빌드 단계 지정

    - start_step: 시작 단계 지정

    - stop_step: 종료 단계 지정

    - verbose: 상세 로깅 활성화

---
### 8.17. Additional builder arguments - 4
##### 기타 빌드 인수 - 2
---

지금까지 이 노트북에서는 예상 보고서 생성까지의 구성만 살펴봤는데, 이러한 빌더 인수의 대부분은 실제로는 FINN 흐름의 후반 단계에서 관련성이 있습니다.

이제 전체 FINN 흐름에 대한 기본 빌드 데이터 흐름 단계를 살펴보겠습니다.

```python
print("\n".join(build_cfg.default_build_dataflow_steps))
```
```bash
#출력
step_qonnx_to_finn
step_tidy_up
step_streamline
step_convert_to_hw
step_create_dataflow_partition
step_specialize_layers
step_target_fps_parallelization
step_apply_folding_config
step_minimize_bit_width
step_generate_estimate_reports
step_hw_codegen
step_hw_ipgen
step_set_fifo_depths
step_create_stitched_ip
step_measure_rtlsim_performance
step_out_of_context_synthesis
step_synthesize_bitfile
step_make_pynq_driver
step_deployment_package
```
<br>


예상 보고서 생성 후 코드 생성 및 IP 생성이 호출되는 것을 볼 수 있습니다(`STEP_HW_CODEGEN` 및 `STEP_HW_IPGEN`).

FIFO 깊이가 결정되고 FIFO가 네트워크에 삽입되면(`STEP_SET_FIFO-DEPTHS`), 각 계층의 IP를 서로 스티칭하여 전체 네트워크의 IP 설계를 생성할 수 있습니다(`STEP_CREATE_STITCHED_IP`).

이 시점에서 더 큰 FPGA 설계 내에 통합할 수 있는 신경망 구현이 완료되면 시뮬레이션(`STEP_MEASURE_RTLSIM_PERFORMANCE`)과 컨텍스트 외 합성(`STEP_OUT_OF_CONTEXT_SYNTHESIS`)을 사용하여 성능 측정을 실행할 수 있습니다.

FINN 빌더는 또한 Zynq 및 Alveo 디바이스에 대한 자동 시스템 통합을 제공하며, 이는 `step_synthesize_bitfile`, `step_make_pynq_driver` 및 `step_deployment_package`를 실행하여 호출할 수 있습니다.

```python
import finn.builder.build_dataflow_steps as build_dataflow_steps
print(build_dataflow_steps.step_hw_codegen.__doc__)
```
```bash
Generate Vitis HLS code to prepare HLSBackend nodes for IP generation.
    And fills RTL templates for RTLBackend nodes.
```
<br>

```python
showSrc(build_dataflow_steps.step_hw_codegen)
```
```bash
def step_hw_codegen(model: ModelWrapper, cfg: DataflowBuildConfig):
    """Generate Vitis HLS code to prepare HLSBackend nodes for IP generation.
    And fills RTL templates for RTLBackend nodes."""

    model = model.transform(PrepareIP(cfg._resolve_fpga_part(), cfg._resolve_hls_clk_period()))
    return model
```
<br>

이것으로 고급 빌더 설정 튜토리얼을 마칩니다. 

아래에서 더 많은 빌더 인수를 조사하고 전체 흐름을 호출하여 비트파일을 생성하는 데 도움이 되는 코드를 찾을 수 있습니다.

---
## 8.18. Example for additional builder arguments & bitfile generation - 1 
---
**추가 빌더 인자 및 비트파일 생성 예제**

##### Standalone Thresholds

FINN에서 컨볼루션은 세 가지 구성 요소로 표현됩니다:
- Im2Col operation

- Matrix multiplication

- MultiThreshold operation

이러한 노드를 HW 레이어로 변환할 때, 기본적으로 MatMul과 MultiThreshold는 매트릭스-벡터-활성화 유닛(MVAU)이라는 **하나**의 컴포넌트로 변환됩니다. 

하지만 FINN 컴파일러를 사용하면 활성화를 별도로 구현할 수 있습니다.

이렇게 하면 독립형 임계값 단위의 폴딩 파라미터를 독립적으로 조정할 수 있기 때문에 커스터마이징의 가능성이 더욱 높아집니다. 

이 기능을 활성화하려면 빌드 인자 `standalone_thresholds`를 `True`로 설정하면 됩니다. 

아래 코드에서 이 기능이 활성화되어 있으며 생성된 .onnx 파일을 확인할 수 있습니다. 

먼저 코드의 주석 처리를 해제해야 한다는 점에 유의하세요.

```python
## Build flow with additional builder arguments enabled
## standalone_thresholds = True

model_dir = os.environ['FINN_ROOT'] + "/notebooks/advanced"
model_file = model_dir + "/end2end_cnv_w2a2_export.onnx"

output_dir = build_dir + "/output_standalone_thresholds"

#Delete previous run results if exist
if os.path.exists(output_dir):
    shutil.rmtree(output_dir)
    print("Previous run results deleted!")

build_steps = [
    custom_step_add_pre_proc,
    custom_step_add_post_proc,
    "step_qonnx_to_finn",
    "step_tidy_up",
    "step_streamline",
    "step_convert_to_hw",
    "step_create_dataflow_partition",
    "step_specialize_layers",
    "step_target_fps_parallelization",
    "step_apply_folding_config",
    "step_minimize_bit_width",
    "step_generate_estimate_reports",
]

cfg_estimates = build.DataflowBuildConfig(
    output_dir            = output_dir,
    mvau_wwidth_max       = 80,
    target_fps            = 10000,
    synth_clk_period_ns   = 10.0,
    fpga_part             = "xc7z020clg400-1",
    standalone_thresholds = True,
    steps                 = build_steps,
    generate_outputs=[
        build_cfg.DataflowOutputType.ESTIMATE_REPORTS,
    ],
)
```
```python
#%%time
#build.build_dataflow_cfg(model_file, cfg_estimates);
```
```python
#showInNetron(build_dir+"/output_standalone_thresholds/intermediate_models/step_generate_estimate_reports.onnx")
```
<br>

#### 8.18. 이해하기

---
## 8.19. Example for additional builder arguments & bitfile generation - 2
---
**추가 빌더 인자 및 비트파일 생성 예제**
###### Run the whole flow**

##### 1. 
아래 코드를 사용하면 전체 빌더 플로우를 호출하고 더 많은 출력물을 얻을 수 있으며, 합성 및 비트파일 생성을 실행하므로 1시간 이상 소요될 수 있습니다. 

먼저 코드의 주석 처리를 해제해야 한다는 점에 유의하세요.

##### 2. 
최적화된 설계를 위해 이 폴더에 [finn-examples](https://github.com/Xilinx/finn-examples)에서 Pynq-Z1 보드의 cnv-w2a2에 대한 폴딩 구성의 로컬 복사본을 저장했습니다. 

그리고 빌드 플로우에 전달합니다.

또한 아래에서는 이제 빌더에 fpga 부분만 전달하는 것이 아니라 보드를 인수로 전달합니다(`board = “Pynq-Z1”`). 

이번에는 생성할 수 있는 모든 출력을 선택합니다. 

전체 빌드를 실행하는 데 몇 시간이 걸릴 수 있다는 점에 유의하세요.

##### 3.
하나의 추가 인수를 설정했습니다: `default_swg_exception = True`. 

이는 이 예제가 Pynq-Z1 보드에 맞게 사용자 정의되었기 때문에 리소스를 최적화하기 위해 불필요한 버퍼링을 피하기 위해 SWG와 MVAU 사이의 FIFO를 수동으로 제거합니다.

```python
import finn.builder.build_dataflow as build
import finn.builder.build_dataflow_config as build_cfg
import os
import shutil

## Build flow with hardware build

model_dir = os.environ['FINN_ROOT'] + "/notebooks/advanced"
model_file = model_dir + "/end2end_cnv_w2a2_export.onnx"

output_dir = build_dir + "/output_bitfile"

#Delete previous run results if exist
if os.path.exists(output_dir):
    shutil.rmtree(output_dir)
    print("Previous run results deleted!")

build_steps = [
    custom_step_add_pre_proc,
    custom_step_add_post_proc,
    "step_qonnx_to_finn",
    "step_tidy_up",
    "step_streamline",
    "step_convert_to_hw",
    "step_create_dataflow_partition",
    "step_specialize_layers",
    "step_target_fps_parallelization",
    "step_apply_folding_config",
    "step_minimize_bit_width",
    "step_generate_estimate_reports",
    "step_hw_codegen",
    "step_hw_ipgen",
    "step_set_fifo_depths",
    "step_create_stitched_ip",
    "step_measure_rtlsim_performance",
    "step_out_of_context_synthesis",
    "step_synthesize_bitfile",
    "step_make_pynq_driver",
    "step_deployment_package",
]

cfg_build = build.DataflowBuildConfig(
    output_dir                    = output_dir,
    mvau_wwidth_max               = 80,
    synth_clk_period_ns           = 10.0,
    #specialize_layers_config_file = "specialize_layers_all_hls.json",
    folding_config_file           = "cnv-w2a2_folding_config.json",
    board                         = "Pynq-Z1",
    shell_flow_type               = build_cfg.ShellFlowType.VIVADO_ZYNQ,
    steps                         = build_steps,
    default_swg_exception         = True,
    generate_outputs=[
        build_cfg.DataflowOutputType.ESTIMATE_REPORTS,
        build_cfg.DataflowOutputType.STITCHED_IP,
        build_cfg.DataflowOutputType.RTLSIM_PERFORMANCE,
        build_cfg.DataflowOutputType.OOC_SYNTH,
        build_cfg.DataflowOutputType.BITFILE,
        build_cfg.DataflowOutputType.PYNQ_DRIVER,
        build_cfg.DataflowOutputType.DEPLOYMENT_PACKAGE,
    ],
)
```
```python
#%%time
#build.build_dataflow_cfg(model_file, cfg_build);
```
<br>

#### 8.19. 이해하기


---