---
title: "[2024반도체해커톤] 6.FINN 예제 이해하기 Advanced 3장  - Custom op"
date: 2024-12-24 16:24:00 +09:00
categories: [Project, 2024_2 반도체해커톤,]
tags:
  [
    verilog,
    systemverilog,
    Vivado,
    Vitis,
    Xilinx,
    FINN,
    Bretivas
  ]
---

## 환경사항
사용하고 있는 환경은 다음과 같다.

| 이름                      | 버전       | 기타 정보 |
| :-------------------------------| :-----------------| :-------: |
| Vivado			| 2023.2		| O	|
| Vitis			| 2023.2		| O	|
| WSL2			| 5.15.167.4	| O	|
| Ubuntu			| 20.04.06	| O	|
| Docker Desktop		|		| O	|


## 목차 
<a href="https://hyeokls.github.io/posts/2024%EB%B0%98%EB%8F%84%EC%B2%B4%ED%95%B4%EC%BB%A4%ED%86%A4-0.WLS2-%ED%99%98%EA%B2%BD%EC%97%90%EC%84%9C-%ED%95%98%EB%93%9C%EC%9B%A8%EC%96%B4-%EC%9E%A5%EB%B9%84-%EC%97%B0%EA%B2%B0%ED%95%98%EA%B8%B0/">0. WSL2 환경에서 하드웨어 장비 연결하기<br>
<a href="https://hyeokls.github.io/posts/2024%EB%B0%98%EB%8F%84%EC%B2%B4%ED%95%B4%EC%BB%A4%ED%86%A4-1.%ED%99%98%EA%B2%BD-%EC%84%A4%EC%A0%95%ED%95%98%EA%B8%B0/">1. 환경 설정하기<br>
<a href="https://hyeokls.github.io/posts/2024%EB%B0%98%EB%8F%84%EC%B2%B4%ED%95%B4%EC%BB%A4%ED%86%A4-2.FINN-%EC%98%88%EC%A0%9C-%EC%9D%B4%ED%95%B4%ED%95%98%EA%B8%B0-Basics-1%EC%9E%A5-How-to-work-with-onnx/">2. FINN 예제 이해하기 Basics 1장 - How to work with onnx<br>
<a href="https://hyeokls.github.io/posts/2024%EB%B0%98%EB%8F%84%EC%B2%B4%ED%95%B4%EC%BB%A4%ED%86%A4-3.FINN-%EC%98%88%EC%A0%9C-%EC%9D%B4%ED%95%B4%ED%95%98%EA%B8%B0-Basics-2%EC%9E%A5-Brevitas-netowrk-import-via-QONNX/">3. FINN 예제 이해하기 Basics 2장 - Brevitas netowrk import via QONNX<br>
<a href="https://hyeokls.github.io/posts/2024%EB%B0%98%EB%8F%84%EC%B2%B4%ED%95%B4%EC%BB%A4%ED%86%A4-4.FINN-%EC%98%88%EC%A0%9C-%EC%9D%B4%ED%95%B4%ED%95%98%EA%B8%B0-Advanced-1%EC%9E%A5-Custom-analysis-pass/">4. FINN 예제 이해하기 Advanced 1장 - Custom analysis pass<br>
<a href="https://hyeokls.github.io/posts/2024%EB%B0%98%EB%8F%84%EC%B2%B4%ED%95%B4%EC%BB%A4%ED%86%A4-5.FINN-%EC%98%88%EC%A0%9C-%EC%9D%B4%ED%95%B4%ED%95%98%EA%B8%B0-Advanced-2%EC%9E%A5-transformation-passes/">5. FINN 예제 이해하기 Advanced 2장 - Transformation passes<br>
<a href="https://hyeokls.github.io/posts/2024%EB%B0%98%EB%8F%84%EC%B2%B4%ED%95%B4%EC%BB%A4%ED%86%A4-6.FINN-%EC%98%88%EC%A0%9C-%EC%9D%B4%ED%95%B4%ED%95%98%EA%B8%B0-Advanced-3%EC%9E%A5-custom-op/">6. FINN 예제 이해하기 Advanced 3장 - Custom op<br>
<a href="https://hyeokls.github.io/posts/2024%EB%B0%98%EB%8F%84%EC%B2%B4%ED%95%B4%EC%BB%A4%ED%86%A4-7.FINN-%EC%98%88%EC%A0%9C-%EC%9D%B4%ED%95%B4%ED%95%98%EA%B8%B0-Advanced-4%EC%9E%A5-Folding/">7. FINN 예제 이해하기 Advanced 4장 - Folding<br>
<a href="https://hyeokls.github.io/posts/2024%EB%B0%98%EB%8F%84%EC%B2%B4%ED%95%B4%EC%BB%A4%ED%86%A4-8.FINN-%EC%98%88%EC%A0%9C-%EC%9D%B4%ED%95%B4%ED%95%98%EA%B8%B0-Advanced-5%EC%9E%A5-Advanced-builder-settings/">8. FINN 예제 이해하기 Advanced 5장 - Advanced builder settings<br>
<a href="https://hyeokls.github.io/posts/2024%EB%B0%98%EB%8F%84%EC%B2%B4%ED%95%B4%EC%BB%A4%ED%86%A4-9.FINN-%EC%98%88%EC%A0%9C-%EC%9D%B4%ED%95%B4%ED%95%98%EA%B8%B0-Cybersecurity-1%EC%9E%A5-Train-mlp-with-brevitas/">9. FINN 예제 이해하기 Cybersecurity 1장 - Train mlp with brevitas<br>
<a href="https://hyeokls.github.io/posts/2024%EB%B0%98%EB%8F%84%EC%B2%B4%ED%95%B4%EC%BB%A4%ED%86%A4-10.FINN-%EC%98%88%EC%A0%9C-%EC%9D%B4%ED%95%B4%ED%95%98%EA%B8%B0-Cybersecurity-2%EC%9E%A5-Import-into-finn-and-verify/">10. FINN 예제 이해하기 Cybersecurity 2장 - Import into finn and verify<br>
<a href="https://hyeokls.github.io/posts/2024%EB%B0%98%EB%8F%84%EC%B2%B4%ED%95%B4%EC%BB%A4%ED%86%A4-11.FINN-%EC%98%88%EC%A0%9C-%EC%9D%B4%ED%95%B4%ED%95%98%EA%B8%B0-Cybersecurity-3%EC%9E%A5-Build-accelerator-with-finn/">11. FINN 예제 이해하기 Cybersecurity 3장 - Build accelerator with finn<br>



# 이번 챕터에서는 무엇을 했나요?
저번 챕터와 마찬가지로 며칠동안은 FINN에서 제공하는 예제를 이해하는 시간을 가졌습니다.

이번 챕터는 FINN의 변환 패스에 대한 개념을 설명하고 예제를 통해 변환의 절차를 공부할 예정입니다.
---
## 6.0.1. Introduction to custom ops in FINN
---
FINN 컴파일러에 새로운 (사용자 정의) 연산 유형을 도입하고 싶다고 가정해 봅시다.

FINN의 사용자 정의 연산은 코드 생성부터 기능 검증에 이르기까지 다양한 용도로 유용합니다. 

이는 특정 인터페이스 사양을 충족하는 사용자 정의 연산을 위한 새로운 Python 모듈을 생성하면 됩니다.

시작하기 전에 한 가지 짚고 넘어가야 할 점은 이러한 사용자 정의 연산은 일반적이며 Vitis HLS나 몇 비트 양자화와는 관련이 없다는 것입니다. 

이 노트북에서 보시겠지만, 사용자 정의 노드에 대해 임의의 Python/C/C++/... 실행 및 코드 생성 경로를 제공할 수 있습니다.
<br>

---
## 6.0.2. The CustomOP base class
---
CustomOp`의 서브클래스는 FINN의 ONNX 노드에 커스텀 기능을 제공하는 방법을 제공합니다.

프레임워크에서 사용되는 모든 커스텀 옵 노드의 베이스 클래스이므로, 실행, 코드 생성 및 기타 기능을 FINN에서 제공하려면 `CustomOp`의 서브클래스를 만들어야 합니다.

먼저 `qonnx` 리포지토리에 있는 `CustomOp` 베이스 클래스 자체를 살펴보겠습니다. 

[여기](https://github.com/fastmachinelearning/qonnx/blob/main/src/qonnx/custom_op/base.py)에서 볼 수 있습니다. 

`finn` 도커 컨테이너에는 이미  `qonnx`가 종속성으로 설정되어 있습니다.

몇 가지 중요한 사항:
1. (Python의) `CustomOp` 인스턴스는 데이터를 저장하는 것이 아니라 ONNX에 저장된 데이터 위에 기능만 제공합니다. 각 `CustomOp` 인스턴스에는 속성을 통해 ONNX `NodeProto`에 액세스할 수 있는 `self.onnx_node` 멤버가 있습니다. 이 프로세스를 더 쉽게 수행할 수 있도록 `CustomOp`에는 사용자 지정 속성 설정자/취득자 시스템도 있습니다.

2. CustomOp` 서브클래스는 아래 메서드를 구현해야 합니다(밑줄로 시작하지 않는 메서드).

3. 사용자 정의 연산자 레지스터에서 검색 가능하려면 `CustomOp` 서브클래스는 `domain` 필드를 해당 서브클래스가 나타나는 파이썬 모듈의 이름으로 설정해야 합니다. 예를 들어, [여기](https://github.com/fastmachinelearning/qonnx/blob/main/src/qonnx/custom_op/general/im2col.py)의 사용자 정의 `Im2Col` 연산 유형을 사용하려면 해당 모듈이 `qonnx/custom_op/general/im2col.py`에 위치하므로 ONNX 노드는 `domain=qonnx.custom_op.general`을 사용해야 합니다.
<br>

```python
from qonnx.custom_op.base import CustomOp
dir(CustomOp)
```
```bash
#출력
['__abstractmethods__',
 '__class__',
 '__delattr__',
 '__dict__',
 '__dir__',
 '__doc__',
 '__eq__',
 '__format__',
 '__ge__',
 '__getattribute__',
 '__gt__',
 '__hash__',
 '__init__',
 '__init_subclass__',
 '__le__',
 '__lt__',
 '__module__',
 '__ne__',
 '__new__',
 '__reduce__',
 '__reduce_ex__',
 '__repr__',
 '__setattr__',
 '__sizeof__',
 '__slots__',
 '__str__',
 '__subclasshook__',
 '__weakref__',
 '_abc_impl',
 'execute_node',
 'get_nodeattr',
 'get_nodeattr_allowed_values',
 'get_nodeattr_def',
 'get_nodeattr_types',
 'infer_node_datatype',
 'make_const_shape_op',
 'make_shape_compatible_op',
 'set_nodeattr',
 'verify_node']
```
<br>

---
## 6.0.3. 이해하기
---
1. 위 코드는 `CustomOp` 클래스의 속성 및 메서드를 설명하고 있으며 각 속성과 메서드는 해당 클래스에서 제공하는 주요 기능들을 나타냅니다.
    - **`CustomOp 클래스의 주요 목적`**:
        - CustomOp는 ONNX 모델에서 사용자 정의 노드를 다룰 때 사용되는 추상 클래스입니다.

        - 모델 내 각 노드의 속성 관리, 데이터 타입 추론, 연산 실행 등을 위한 표준화된 인터페이스를 제공합니다.

        - 이 클래스는 특정 노드에 대한 사용자 정의 연산을 구현하는 데 사용됩니다.

2. CustomOp 클래스는 모든 커스텀 연산 노드를 기반으로 하며 각 커스텀 노드가 가져야 할 다양한 기능을 포함하고 있습니다. 


---
### 6.1. A Simple CustomOp Example
---
입력값을 지정된 지수(속성으로 지정됨)로 올리는 간단한 CustomOp를 만들어 보겠습니다. 

```python
from onnx import helper
import numpy as np

class MyPythonPowerOp(CustomOp):
    
    # here we use the CustomOp attribute system to make it easier
    # to set/get custom attributes on this node
    def get_nodeattr_types(self):
        return {
            # each entry is:
            # name of attribute : (dtype, required, default value)
            # dtype follows the ONNX attribute protobuf so
            # "i" is int, "s" is string, "f" is float,
            # "ints" is a list of integers...
            # also good practice to document what each attribute does here:
            
            # which integer power to raise the input to
            "exponent" : ("i", True, 0),
            # execution mode : currently only python
            "exec_mode" : ("s", True, "python"),
        }
    
    # return an ONNX node that has the same shape inference behavior
    # here we want in shape = out shape, so we use the ONNX ReLU
    # node to mimic its shape inference behavior
    # we have access to the entire ModelWrapper to help make this decision
    # (the parameter called model)
    def make_shape_compatible_op(self, model):
        node = self.onnx_node
        # make a Relu node connected to the same in-out tensors to get
        # shape inference
        # a general-purpose alternative is to use a Constant node that 
        # produces the desired shape
        return helper.make_node("Relu", [node.input[0]], [node.output[0]])

    # used for FINN DataType inference: set the output tensors' datatypes
    # accordingly for this node
    # here we assume input datatype = output datatype
    # we have access to the entire ModelWrapper to help make this decision
    # (the parameter called model)
    def infer_node_datatype(self, model):
        node = self.onnx_node
        # data type stays the same
        dtype = model.get_tensor_datatype(node.input[0])
        model.set_tensor_datatype(node.output[0], dtype)
    
    # execute this node
    # context: used for both input and output, dictionary of named
    #          tensors
    # graph: the ONNX GraphProto (ModelWrapper.graph), generally 
    #         not needed to execute a single node
    def execute_node(self, context, graph):
        exec_mode = self.get_nodeattr("exec_mode")
        if exec_mode == "python":
            # get names of node input and output tensors
            i_name = self.onnx_node.input[0]
            o_name = self.onnx_node.output[0]
            # grab input tensor from context
            i_tensor = context[i_name]
            # get which power to raise to from attribute
            expnt = self.get_nodeattr("exponent")
            # compute and put output into context
            o_tensor = np.power(i_tensor, expnt)
            context[o_name] = o_tensor
        else:
            raise Exception("Only python exec_mode is supported")
        
    # can use to do a sanity check of all the node's properties
    # optional, not implemented here
    def verify_node(self):
        pass
        
```
<br>

사용자 정의 연산자를 사용할 수 있도록 하려면 이를 등록해야 합니다.

이를 위한 가장 좋은 방법은 문자열(연산자 이름)을 클래스(연산자 구현)에 매핑하는 `custom_op` 사전을 포함하는 `qonnx.custom_op` 아래에 서브모듈을 만드는 것입니다. 
 
여기서는 Jupyter 노트북에 있으므로 런타임에 다음과 같이 하이재킹하겠습니다:

```python
import qonnx.custom_op.general as general
general.custom_op["MyPythonPowerOp"] = MyPythonPowerOp
```
<br>

사전을 보면 이 하위 모듈에 어떤 사용자 정의 작업이 등록되어 있는지 확인할 수 있습니다:

```python
general.custom_op
```
<br>

---
## 6.2. Let's Try Out oure CustomOp
---
일부 기능을 시험해보기 위해 노드가 포함된 작은 ONNX 그래프를 수동으로 작성하겠습니다.

1. 이 작업은 일반적으로 이 CustomOp의 단위 테스트에 포함됩니다. 

2. 그래프는 먼저 입력/출력 텐서 정보(이름, 유형, 모양)를 지정하여 작성합니다.

3. 그런 다음 사용자 정의 노드가 생성되며, 이 노드는 나중에 입력/출력 텐서 정보를 따라 그래프를 생성하는 데 사용됩니다. 

4. 그래프를 사용해 모델을 구축합니다. 

5. 마지막으로 FINN의 ModelWrapper 함수를 사용하여 모델을 감싸줍니다.

```python
from qonnx.core.modelwrapper import ModelWrapper
from onnx import TensorProto
from qonnx.util.basic import qonnx_make_model

def make_graph(ishape, exp, op_type = "MyPythonPowerOp"):
    inp = helper.make_tensor_value_info(
        "inp", TensorProto.FLOAT, ishape
    )
    outp = helper.make_tensor_value_info(
        "outp", TensorProto.FLOAT, ishape
    )

    custom_node = helper.make_node(
        # op type string in ONNX, what we used to register the custom op
        op_type,
        # name of input tensor
        ["inp"],
        # name of output tensor
        ["outp"],
        # specify domain s.t. FINN can find our op under this submodule
        domain="qonnx.custom_op.general",
        # set up attributes
        exponent = int(exp),
        exec_mode = "python"
    )

    graph = helper.make_graph(
        nodes=[custom_node], name="custom_graph", inputs=[inp], outputs=[outp]
    )
    model = qonnx_make_model(graph, producer_name="custom-model")
    return ModelWrapper(model)
```
<br>

#### 6.2 이해하기

이 코드는 사용자 정의 연산을 포함한 작은 ONNX 그래프를 생성하고 FINN의 ModelWrapper로 감싸는 함수입니다.

1. **`make_graph`**:
    - 입력 텐서의 모양과 사용자 정의 연산의 속성을 지정하여 ONNX 그래프를 생성합니다.

    - 입력 및 출력 텐서를 생성합니다. 

2. **`사용자 노드 생성`**:
    - helper.make_node: 그래프에 포함될 노드를 생성합니다.
    - op_type: 사용자 정의 연산 유형.
    - inputs: 입력 텐서의 이름 리스트.
    - outputs: 출력 텐서의 이름 리스트.
    - domain: FINN에서 사용자 정의 연산을 식별할 수 있도록 지정.
    - 속성(attribute):
        - exponent: 노드의 지수 속성 값.
        
    - exec_mode: 실행 모드 (현재 "python"만 지원).

3. **`ONNX 그래프 생성`**:
    - 노드 리스트와 입력/출력 텐서 정보를 사용해 그래프를 만듭니다.

4. **`모델 생성 및 감싸기`**
    - qonnx_make_model: ONNX 모델 객체를 생성.

    - ModelWrapper: FINN의 유틸리티로 모델을 감싸 후속 작업을 간단히 처리할 수 있도록 지원.
<br>

이제 입력 텐서 모양을 지정하고 방금 만든 함수를 사용하여 그래프를 생성합니다. 

입력 텐서 모양과 지수 값은 파라미터로 전달됩니다. 

이 매개변수는 `MyPythonPowerOp` 연산을 사용하여 모델, 그래프 및 사용자 정의 노드를 생성하는 데 사용됩니다.

---
## 6.3. 이어서 하기!
---
```python
# generate a small graph with our custom op
input_shape = (1, 2, 4)
ret_model = make_graph(input_shape, 2)
ret_model.model.graph.node
```
```bash
#결과
[input: "inp"
output: "outp"
op_type: "MyPythonPowerOp"
attribute {
  name: "exec_mode"
  s: "python"
  type: STRING
}
attribute {
  name: "exponent"
  i: 2
  type: INT
}
domain: "qonnx.custom_op.general"
]
```
<br>

앞서 정의한 `input_shape`를 기반으로 랜덤 텐서를 생성합니다.

아래 `random_input`의 모양과 값, 데이터 타입을 참조하세요. 


```python
from qonnx.core.datatype import DataType
from qonnx.util.basic import gen_finn_dt_tensor

# generate a random input of e.g signed 4-bit values
random_input = gen_finn_dt_tensor(DataType["INT4"], input_shape)
random_input
```
```bash
#결과
array([[[ 5., -7., -6.,  5.],
        [ 6., -7.,  6., -1.]]], dtype=float32)
```
<br>


방금 생성한 임의의 값으로 입력 사전을 생성하기만 하면 됩니다. 

그런 다음 방금 생성한 모델과 임의의 값을 사용하여 모델을 실행합니다. 

```python
from finn.core.onnx_exec import execute_onnx

# run with FINN's execute_onnx
inp_dict = {"inp" : random_input}
ret = execute_onnx(ret_model, inp_dict)
ret
```
```bash
#결과
{'outp': array([[[25., 49., 36., 25.],
         [36., 49., 36.,  1.]]], dtype=float32)}
```
<br>


완료! 사용자 지정 연산을 사용하는 모델을 방금 실행했습니다.

결과는 입력된 숫자를 2의 거듭제곱한 값이어야 합니다.

#### 6.3. 이해하기
1. `input_shape = (1,2,4)` 및 `ret_model = make_graph(input_shape,2)` 를 통하여 입력 텐서의 크기 및 지수 연산 숫자를 정하여 모델을 생성하였습니다.

2. `DataType` 및 `gen_finn_dt_tensor` 를 임포트 해준 뒤 INT4 타입과 입력 텐서를 넣어 랜덤 텐서를 생성합니다.

3. `execute_onnx`를 임포트하여 ONNX를 실행할 수 있는 환경을 만들어 주었고 `inp_dict`으로 입력 데이터 딕셔너리를 생성합니다.

4. 또한 `ret = execute_onnx(ret_model, inp_dict)` 을 사용하여 결과값을 입력 데이터를 기반으로 결과를 반환하였고 값을 확인합니다.


---
## 6.4. A CustomOp with C++ Generation
---
### C++로 생성하는 CustomOp

예를 들어 `CutomOp`을 C++로 작성하고 이전에 했던 것과 동일한 방식으로 모델을 생성할 수 있습니다. 

이 작업은 `Python`에서 C++ 코드를 호출할 수 있는 `Python bindings`을 통해 수행할 수 있습니다.

실제로 C++ 코드를 컴파일하고 파이썬에서 실행해 보겠습니다. 

다음 클래스는 이전에 작성한 `MyPythonPowerOp` 클래스를 기반으로 합니다. 

생성된 C++ 코드, 빌드 스크립트 및 실행 가능한 애플리케이션의 디렉터리를 지정하는 `get_nodeattr_types` 함수에 `codegen_dir` 속성을 새로 추가합니다.

C++ 코드를 파일에 쓰고 빌드하는 `my_custom_cpp_gen`이라는 새 함수를 정의합니다. 

마지막으로 커스텀옵의 C++ 실행을 지원하도록 `execute_node` 함수를 수정합니다. 

if-else 문의 `c++` 브랜치는 먼저 입력 텐서를 플랫화하여 “input.txt” 파일에 씁니다.

그런 다음 bash 명령을 사용하여 C++ 컴파일된 애플리케이션이 실행됩니다. 

애플리케이션은 “.txt” 파일을 읽고 지수를 기반으로 거듭제곱 값을 계산한 다음 그 결과를 “output.txt” 파일에 다시 씁니다. 

그런 다음 출력 파일의 결과를 읽고 원래 모양으로 다시 변형합니다. 

마지막으로 결과가 'context' 사전에 기록됩니다.

```python
from finn.util.basic import make_build_dir, CppBuilder
import subprocess

# derive from our previous example
class MyMixedPowerOp(MyPythonPowerOp):
    
    # here we use the CustomOp attribute system to make it easier
    # to set/get custom attributes on this node
    def get_nodeattr_types(self):
        return {
            # each entry is:
            # name of attribute : (dtype, required, default value)
            # dtype follows the ONNX attribute protobuf so
            # "i" is int, "s" is string, "f" is float,
            # "ints" is a list of integers...
            # also good practice to document what each attribute does here:
            
            # which integer power to raise the input to
            "exponent" : ("i", True, 0),
            # execution mode : python or c++
            "exec_mode" : ("s", True, "python"),
            # code generation directory
            "codegen_dir" : ("s", False, ""),
        }
    
    def my_custom_cpp_gen(self):
        codegen_dir = make_build_dir(prefix="my_custom_op")
        # set attribute for codegen dir
        self.set_nodeattr("codegen_dir", codegen_dir)
        # generate some C++ code
        cpp_code = """
#include <iostream>
#include <fstream>
using namespace std;
#define EXPONENT %d

int main(int argc, char **argv) {
    ifstream infile("input.txt");
    ofstream outfile("output.txt");
    
    float elem;
    while (infile >> elem)
    {
        float res = 1.0;
        for(int i=0; i < EXPONENT; i++) {
            res *= elem;
        }
        outfile << res << "\\n";
    }

    return 0;
}
        """ % (self.get_nodeattr("exponent"))
        with open(codegen_dir+"/top.cpp", "w") as f:
            f.write(cpp_code)
        builder = CppBuilder()
        # to enable additional debug features please uncommand the next line
        builder.append_includes("--std=c++11")
        builder.append_includes("-O3")
        builder.append_sources(codegen_dir + "/*.cpp")
        builder.set_executable_path(codegen_dir + "/node_model")
        builder.build(codegen_dir)
    
    # execute this node
    # context: used for both input and output, dictionary of named
    #          tensors
    # graph: the ONNX GraphProto (ModelWrapper.graph), generally 
    #         not needed to execute a single node
    def execute_node(self, context, graph):
        exec_mode = self.get_nodeattr("exec_mode")
        # get names of node input and output tensors
        i_name = self.onnx_node.input[0]
        o_name = self.onnx_node.output[0]
        # grab input tensor from context
        i_tensor = context[i_name]
        # get which power to raise to from attribute
        expnt = self.get_nodeattr("exponent")
        if exec_mode == "python":
            # compute and put output into context
            o_tensor = np.power(i_tensor, expnt)
            context[o_name] = o_tensor
        elif exec_mode == "c++":
            build_dir = self.get_nodeattr("codegen_dir")
            # save input as txt, could preprocess, change layout etc..
            np.savetxt(build_dir+"/input.txt", i_tensor.flatten())
            bash_command = ["./node_model"]
            proc_run = subprocess.Popen(bash_command, cwd=build_dir, stdout=subprocess.PIPE)
            proc_run.communicate()
            o_tensor = np.loadtxt(build_dir+"/output.txt")
            o_tensor = o_tensor.reshape(i_tensor.shape)
            context[o_name] = o_tensor
        else:
            raise Exception("Only python and c++ exec_mode is supported")
        
    # can use to do a sanity check of all the node's properties
    # optional, not implemented here
    def verify_node(self):
        pass
        
```
<br>

#### 6.4 이해하기
위 코드가 길기 때문에 부분을 나눠서 이해해보려고 하였습니다.

##### 선언 파트 
```python
from finn.util.basic import make_build_dir, CppBuilder
import subprocess
```
- `make_build_dir: `
    - `Build_dir`로 사용할 접두사가 지정된 폴더를 만듭니다. 

    - `tempfile`, `mkdtmep` 대신 이함수를 사용하여 생성된 파일이 `PIN Docker` 컨테이너가 종료된 후 호스트에서 살아남도록 합니다.

- `CppBuider: `
    - g++ 컴파일러 명령어를 빌드하여 `code_gen_dir` 에서 c++ 코드의 실행 파일을 생성하고, 이는 이 클래스의 function build()로 전달됩니다.

- `subprocess`는 쉘 명령 등 다른 프로세스를 실행하는데 도움을 줍니다.

2. C ++ 코드 생성 및 컴파일 파트
```python
class MyMixedPowerOp(MyPythonPowerOp):

    def get_nodeattr_types(self):
        return {
            # which integer power to raise the input to
            "exponent" : ("i", True, 0),
            # execution mode : python or c++
            "exec_mode" : ("s", True, "python"),
            # code generation directory
            "codegen_dir" : ("s", False, ""),
        }
    
    def my_custom_cpp_gen(self):
        codegen_dir = make_build_dir(prefix="my_custom_op")
        # set attribute for codegen dir
        self.set_nodeattr("codegen_dir", codegen_dir)
        # generate some C++ code
        cpp_code = """
#include <iostream>
#include <fstream>
using namespace std;
#define EXPONENT %d

int main(int argc, char **argv) {
    ifstream infile("input.txt");
    ofstream outfile("output.txt");
    
    float elem;
    while (infile >> elem)
    {
        float res = 1.0;
        for(int i=0; i < EXPONENT; i++) {
            res *= elem;
        }
        outfile << res << "\\n";
    }

    return 0;
}
        """ % (self.get_nodeattr("exponent"))
        with open(codegen_dir+"/top.cpp", "w") as f:
            f.write(cpp_code)
        builder = CppBuilder()
        # to enable additional debug features please uncommand the next line
        builder.append_includes("--std=c++11")
        builder.append_includes("-O3")
        builder.append_sources(codegen_dir + "/*.cpp")
        builder.set_executable_path(codegen_dir + "/node_model")
        builder.build(codegen_dir)

```
<br>

1. `MyPythonPowerOp` 클래스를 상속하여 Python 과 C++에서 Custom 연산을 수행할 수 있도록 만들기 위해 `MyMixedPowerOp` 클래스를 생성합니다.

2. `get_nodeattr_types` 메서드는 노드 속성을 정의하는 메서드로, 특정 속성의 유형, 필요여부, 기본값 등을 지정하는 역할을 합니다.
    - 여기서는 거듭제곱, 실행모드, 저장될 경로을 지정하였습니다.

3. `my_custom_cpp_gen(self)` 메서드는 C++ 코드를 생성하고 컴파일하여 실행 가능한 바이너리를 만드는 역할을 합니다.
    - `codegen_dir` 를 통하여 디렉토리를 생성합니다.

    - `self.set_nodeattr` 를 통하여 생성된 디렉토리 경로를 노드 속성에 저장합니다. 

    - `cpp_code=` 를 통하여 문자열로 정의합니다. 

4. `with open() as f:` 를 통하여 C++ 코드를 특정 디렉토리에 .cpp 파일로 저장합니다.

5. `blilder = CppBuilder()` 로 사용하여 C++ 컴파일러를 호출합니다.
    - 여러가지 옵션을 추가한 뒤 `.build()` 를 사용하여 C++ 코드를 컴파일 합니다.

##### 모드 선택 파트  
```python
    def execute_node(self, context, graph):
        exec_mode = self.get_nodeattr("exec_mode")
        # get names of node input and output tensors
        i_name = self.onnx_node.input[0]
        o_name = self.onnx_node.output[0]
        # grab input tensor from context
        i_tensor = context[i_name]
        # get which power to raise to from attribute
        expnt = self.get_nodeattr("exponent")
        if exec_mode == "python":
            # compute and put output into context
            o_tensor = np.power(i_tensor, expnt)
            context[o_name] = o_tensor
        elif exec_mode == "c++":
            build_dir = self.get_nodeattr("codegen_dir")
            # save input as txt, could preprocess, change layout etc..
            np.savetxt(build_dir+"/input.txt", i_tensor.flatten())
            bash_command = ["./node_model"]
            proc_run = subprocess.Popen(bash_command, cwd=build_dir, stdout=subprocess.PIPE)
            proc_run.communicate()
            o_tensor = np.loadtxt(build_dir+"/output.txt")
            o_tensor = o_tensor.reshape(i_tensor.shape)
            context[o_name] = o_tensor
        else:
            raise Exception("Only python and c++ exec_mode is supported")
        
    # can use to do a sanity check of all the node's properties
    # optional, not implemented here
    def verify_node(self):
        pass
        
```
<br>

1. `excute_node` 메서드를 사용하여 거듭제곱 연산을 수행하고 모드를 선택할 수 있게 합니다.
    - `context` 에는 입력과 출력 텐서를 저장합니다.
    - `graph` 는 ONNX 모델의 그래프 겍체를 의미합니다.

2. `exec_mode` 는 Python 또는 C++ 중 하나를 선택합니다.

3. `i_name, o_name` 으로 입력과 출력 텐서의 이름을 추출합니다.

4. `expnt` 는 제곱수 파트입니다.

5. `i_tensor` 는 `context` 에서 입력 텐서를 가져옵니다.

6. 만약 Python Mode 라면 `np.power` 를 사용하여 거듭제곱을 진행한 뒤 결과를 `o_tensor` 에 저장합니다.

7. 만약 C++ Mode 라면 `codegen_dir` 에서 C++ 코드가 생성된 디렉토리 경로를 가져온 뒤 
    - 입력 텐서를 `flatten`한 뒤 `.txt` 파을로 저장을 합니다.

    - `bash_command` 및 `subprocess.Popen`을 통해 실행파일을 실행한 뒤 `.txt` 를 읽어 연산한 뒤 다시 `.txt`로 저장합니다.

    - `o_tensor` 에 값을 저장한 뒤 원래 텐서로 복원한 뒤 최종적으로 `context` 에 저장합니다.

---
## 6.5. 이어서
---
이전과 같은 방식으로 새 CustomOp을 등록합니다. 

그런 다음 이전과 동일한 함수 `make_graph`를 사용하여 다른 그래프를 만듭니다.

아래에 사용자 정의 연산이 포함된 노드가 인쇄된 것을 볼 수 있습니다.

```python
# register our new op
general.custom_op["MyMixedPowerOp"] = MyMixedPowerOp

# make graph with new op
mixedop_graph = make_graph(input_shape, 2, op_type = "MyMixedPowerOp")
mixedop_graph.graph.node
```
<br>

CustomOp 내의 모든 함수, 기본 C++ 코드 디렉토리 및 `exec_mode` 속성을 인쇄하기만 하면 됩니다.

```bash
#출력
[input: "inp"
output: "outp"
op_type: "MyMixedPowerOp"
attribute {
  name: "exec_mode"
  s: "python"
  type: STRING
}
attribute {
  name: "exponent"
  i: 2
  type: INT
}
domain: "qonnx.custom_op.general"
]

```
<br>

```python
from qonnx.custom_op.registry import getCustomOp

# get FINN wrapper for this node, with all the functionality
op_inst = getCustomOp(mixedop_graph.model.graph.node[0])
print("Available functions: " + str(dir(op_inst)))
# query some attributes
print("codegen_dir: " + op_inst.get_nodeattr("codegen_dir"))
print("exec_mode: " + op_inst.get_nodeattr("exec_mode"))
```
```bash
#결과
Available functions: ['__abstractmethods__', '__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__slots__', '__str__', '__subclasshook__', '__weakref__', '_abc_impl', 'execute_node', 'get_nodeattr', 'get_nodeattr_allowed_values', 'get_nodeattr_def', 'get_nodeattr_types', 'infer_node_datatype', 'make_const_shape_op', 'make_shape_compatible_op', 'my_custom_cpp_gen', 'onnx_node', 'onnx_opset_version', 'set_nodeattr', 'verify_node']
codegen_dir: 
exec_mode: python

```
<br>

#### 6.5. 이해하기
1. `qonnx.custom_op.registry` 를 통하여 주어진 ONNX 노드에 대한 QONNX Custom OP 인스턴스가 있는 경우 반환합니다.

2. 커스텀 연산을 등록한 뒤 `op_tpye` 까지 설정을 하여 그래프를 생성합니다.
    - 출력된 결과물을 보면 잘 된 것을 확인할 수 있습니다. 

3. `getCustomOp` 를 사용하여 지정된 노드를 감싸는 FINN의 커스텀 연산자 래퍼 객체를 반환합니다.
    - 그래프의 첫 번째 노드를 선택합니다.
    
    - 반환된 노드로 같은 것임을 확인할 수 있습니다.

    - 경로 및 Mode 를 곽인할 수 있습니다.
<br>
---
## 6.6.1. Implement a code generation 1_Transformation
---
지정된 노드의 속성에 액세스하고 수정하여 특정 모델을 변환하는 로컬 변환 함수를 정의합니다.

“codegen_dir"이 존재하지 않는 경우 ‘MyMixedPowerOp’ 노드에서 `my_custom_cpp_gen` 함수를 실행합니다.

```python
#from qonnx.transformation.base import Transformation
# can derive from NodeLocalTransformation for faster (parallel) execution
from qonnx.transformation.base import NodeLocalTransformation
import os

class MyNodeLocalCodeGen(NodeLocalTransformation):
    
    # will get called (possibly in parallel) for each node
    def applyNodeLocal(self, node):
        # keep track whether we changed anything
        modified_graph = False
        # check node type before we do anything
        if node.op_type == "MyMixedPowerOp":
            # get FINN wrapper for this node, with all the functions
            op_inst = getCustomOp(node)
            if not os.path.isdir(op_inst.get_nodeattr("codegen_dir")):
                # call the codegen function we defined
                # this will modify the underlying node by setting attribute
                op_inst.my_custom_cpp_gen()
                # codegen function modifies attribute
                modified_graph = True
        # important: must return modified_graph = False at some point
        # otherwise transformation will run in infinite loop!
        return (node, modified_graph)
```
<br>

이전에 가지고 있던 모델에 변환을 적용합니다. 

반환된 모델은 지정된 변환을 적용한 후의 입력 모델과 동일한 모델입니다. 

```python
mixedop_graph_new = mixedop_graph.transform(MyNodeLocalCodeGen())
```
<br>

CustomOp 노드에서 “codegen_dir” 속성을 인쇄합니다.

```python
new_op_inst = getCustomOp(mixedop_graph_new.graph.node[0])
codegen_dir = new_op_inst.get_nodeattr("codegen_dir")
print(codegen_dir)
```
코드 생성 폴더에 컴파일 스크립트, 컴파일된 애플리케이션 및 C++ 소스 파일이 포함되어 있음을 알 수 있습니다:

```bash
#결과
/tmp/finn_dev_lsh/my_custom_op8j7du94t
```
<br>

```python
! ls {codegen_dir}
```
<br>

C++ 소스 파일의 내용을 확인해 보겠습니다:

```bash
compile.sh  node_model	top.cpp
```
<br>

```python
! cat {codegen_dir}/top.cpp
```
```c++
//결과
#include <iostream>
#include <fstream>
using namespace std;
#define EXPONENT 2

int main(int argc, char **argv) {
    ifstream infile("input.txt");
    ofstream outfile("output.txt");
    
    float elem;
    while (infile >> elem)
    {
        float res = 1.0;
        for(int i=0; i < EXPONENT; i++) {
            res *= elem;
        }
        outfile << res << "\n";
    }

    return 0;
}
    
```
<Br>

#### 6.6. 이해하기

1. `NodeLocalTransforamtion` 를 사용하였는데 이는 노드 단위로 변환을 수행할 수 있는 FINN의 기본 클래스이며,  병렬 처리를 지원합니다.
    - 이를 서브캐싱하는 변환은 추상 메서드인 `applyNodeLocal()` 을 구현해야합니다.

    - 변환이 여러 번 호출될 때 model_was_changed=false를 반환해야 합니다. 그렇지 않으면 approx_repeated()는 무한히 반복됩니다.

2. `applyNodeLocal` 메서드를 통해 그래프 내의 각 노드에 대해 호출을 하고 노드의 `op_tpye` 을 확인한 뒤 그 조건에 따라 변환 작업을 진행합니다.

3. `modified_graph = False` 를 통하여 현재 노드에서 그래프가 변경되었는지 여부를 기록하고 변경된 경우에는 `True` 로 설정합니다.

4. `if` 를 통하여 현재 노드의 연산 타입을 확인합니다.

5. `op_inst` 를 통하여 가져옵니다.

6. `if not` 을 통하여 디렉토리의 존재 여부를 확인합니다.
    - 디렉토리가 없으면 C++ 코드 생성을 수행합니다.

    - `op_inst.my_custom_cpp_gen()` 을 통하여 코드 생성을 호출합니다. 
        - 이때 그래프가 변경되기 때문에 `True`로 설정됩니다. 

7. 최종적으로 값을 반환합니다. 

8. `mixedop_graph_new` 를 통하여 새 그래프를 반환 받습니다.
<Br>

---
### 6.7. Manually generate input and run C++ node model 
---
이제 입력 데이터를 수동으로 생성하여 `input.txt` 파일에 기록하겠습니다. 

그런 다음 컴파일된 애플리케이션을 수동으로 실행하고 마지막으로 `output.txt` 파일에서 결과를 확인합니다. 

이 작업의 목적은 대부분 FINN이 커스텀 작업을 실행할 때 '마법'이 일어나지 않고 단지 프로그램을 실행하는 것임을 보여주기 위한 것입니다.

사용자 정의 연산 실행을 디버깅할 때 이 점을 염두에 두는 것이 좋습니다.

예를 들어, 여기서 `gdb`를 사용하여 C++ 노드 모델의 내부를 디버깅할 수 있습니다.

```python
! echo "7.0 8.0 9.0" > {codegen_dir}/input.txt
! cd {codegen_dir}; ./node_model
! cat {codegen_dir}/output.txt
! rm {codegen_dir}/*.txt
```
```bash
# cat 결과
49
64
81
```

#### 6.7. 이해하기
1. `input.txt` 에 값을 수동으로 기록합니다. 

2. `cd` 명령으로 디렉토리를 이동한 뒤 명령을 실행합니다.

3. `cat` 명령어를 통해 출력값을 확인합니다.

4. `rm` 명령어로 `.txt` 파일들을 삭제합니다.

---
## 6.8. Use FINN execution follows
---
이제 FINN 내부에서 커스텀 노드 실행을 트리거하겠습니다.

커스텀 노드가 ONNX 그래프 내에서 발견되면 적절한 핸들러(이 경우 컴파일된 C++ 프로그램 실행)를 자동으로 실행하는 커스텀 ONNX 실행 기능을 통해 실행해 보겠습니다.

이를 위해 먼저 미리 지정된 텐서 모양을 가진 임의의 텐서를 생성하고 이를 출력합니다. 

```python
# generate a random input of e.g signed 4-bit values
random_input = gen_finn_dt_tensor(DataType["INT4"], input_shape)
random_input
```
```bash
#결과
array([[[ 2., -8., -2., -7.],
        [-8., -3.,  5.,  4.]]], dtype=float32)
```
<br>

CustomOp 노드 속성을 '파이썬' 모드에서 실행하도록 설정합니다.

그런 다음 무작위 입력 텐서를 사용하여 입력 딕셔너리를 생성하고 `execute_onnx`를 사용하여 변환된 모델을 실행합니다.

결과를 확인하기 위해 출력을 인쇄합니다.

```python
# run with FINN's execute_onnx, custom node will use Python execution
new_op_inst.set_nodeattr("exec_mode", "python")
inp_dict = {"inp" : random_input}
ret = execute_onnx(mixedop_graph_new, inp_dict)
ret
```
```bash
#결과
{'outp': array([[[ 4., 64.,  4., 49.],
         [64.,  9., 25., 16.]]], dtype=float32)}
```
<br>

'C++' 실행 모드에서 이전 프로세스를 반복합니다. 

```python
# run with FINN's execute_onnx, custom node will use c++ execution
new_op_inst.set_nodeattr("exec_mode", "c++")
ret = execute_onnx(mixedop_graph_new, inp_dict)
ret
```
```bash
#결과
{'outp': array([[[ 4., 64.,  4., 49.],
         [64.,  9., 25., 16.]]])}
```
<br>

#### 6.8. 이해하기
1. 무작위로 입력 텐서를 생성합니다. 그 결과로 값들이 출력됩니다.

2. `python Mode` 로 실행 한 뒤 출력 값을 확인합니다. 

3. `C++ Mode` 로 실행 한 뒤 출력 값을 확인합니다.

**위 두 값이 동일한 것으로 보아 정상적으로 작성됨을 확인할 수 있습니다.**

---