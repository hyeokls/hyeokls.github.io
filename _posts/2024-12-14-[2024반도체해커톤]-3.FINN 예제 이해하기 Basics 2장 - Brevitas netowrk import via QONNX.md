---
title: "[2024반도체해커톤] 3.FINN 예제 이해하기 Basics 2장 - Brevitas netowrk import via QONNX"
date: 2024-12-14 02:20:00 +09:00
categories: [Project, 2024_2 반도체해커톤,]
tags:
  [
    verilog,
    systemverilog,
    Vivado,
    Vitis,
    Xilinx,
    FINN,
    Bretivas
  ]
---

## 환경사항
사용하고 있는 환경은 다음과 같다.

| 이름                      | 버전       | 기타 정보 |
| :-------------------------------| :-----------------| :-------: |
| Vivado			| 2023.2		| O	|
| Vitis			| 2023.2		| O	|
| WSL2			| 5.15.167.4	| O	|
| Ubuntu			| 20.04.06	| O	|
| Docker Desktop		|		| O	|


## 목차 
0. WSL2 환경에서 하드웨어 장비 연결하기
1. 환경 설정하기
2. FINN 예제 이해하기 1편 - How to used finn
3. FINN 예제 이해하기 2편 - Brevitas network import via QONNX

# 3.0 이번 챕터에서는 무엇을 했나요?
저번 챕터와 마찬가지로 며칠동안은 FINN에서 제공하는 예제를 이해하는 시간을 가졌습니다.

이번 챕터는 brevitas 에 대해 공부할 예정입니다.

# 3.1. QONNX 교환 형식을 사용하여 Brevitas 네트워크를 FINN으로 가져오기
**참고: 이전에는 Brevitas에서 FINN-ONNX 교환 포맷을 직접 내보내어 FINN 컴파일러로 전달할 수 있었습니다. 이 지원은 더 이상 사용되지 않으며 FINN은 프런트엔드로 QONNX 형식으로 내보내기를 사용하지만, 내부적으로는 여전히 FINN-ONNX 형식을 사용합니다.**

<br>

## 3.1 예제 목표
---
1. 훈련된 PyTorch 모델을 로드합니다.
<br>
2. 브레비타스 QONNX 내보내기를 호출하고 Netron으로 시각화합니다.
<br>
3. FINN으로 임포트하고 QONNX를 FINN-ONNX로 변환합니다.

<br>

### 3.1.1.
---
다음 유틸리티 함수를 사용하여 함수 호출에 대한 소스 코드를 인쇄하고(showSrc()) 주피터 노트북에서 Netron을 사용하여 네트워크를 시각화합니다(showInNetron()): 
```python
import onnx
from finn.util.visualization import showSrc, showInNetron
```
<br>

### 3.1.2. 학습된 PyTorch 모델 로드하기
---
FINN Docker 이미지는 여러 [[예제 Brevitas 네트워크](https://github.com/Xilinx/brevitas/tree/master/src/brevitas_examples/bnn_pynq)]와 함께 제공되며, 여기서는 LFC-w1a1 모델을 예제 네트워크로 사용합니다.

이것은 MNIST 데이터 세트에서 학습된 이진화된 완전 연결 네트워크입니다. 

먼저 PyTorch 네트워크 정의가 어떻게 생겼는지 살펴보겠습니다:

<br>

```python
from brevitas_examples import bnn_pynq
showSrc(bnn_pynq.models.FC)
```


- `showSrc(bnn_pynq.models.FC)`는 Brevitas 프레임워크에서 `bnn_pynq`라는 예제 모듈 내의 `FC` (Fully Connected) 모델의 소스 코드를 확인하는 명령입니다.

- 이 명령은 Brevitas 프레임워크에서 특정 객체나 클래스를 나타내는 `FC` 모델의 소스를 출력하기 위한 함수로, `showSrc()`는 일반적인 Python 내장 함수가 아니므로 `showSrc` 함수를 직접 사용하려면 그 정의가 필요합니다.

위 명령어를 실행하면 다음과 같이 나오게 됩니다.
<br>

```python
# Copyright (C) 2023, Advanced Micro Devices, Inc. All rights reserved.
# SPDX-License-Identifier: BSD-3-Clause

import ast
from functools import reduce
from operator import mul

import torch
from torch.nn import BatchNorm1d
from torch.nn import Dropout
from torch.nn import Module
from torch.nn import ModuleList

from brevitas.nn import QuantIdentity
from brevitas.nn import QuantLinear

from .common import CommonActQuant
from .common import CommonWeightQuant
from .tensor_norm import TensorNorm
```
|코드 |                                                 설명 |
|:-------------|:--------------------------------------------------- |
|import ast|문자열 형태의 리스트나 딕셔너리를 실제 Python 객체로 변환하는 데 사용됩니다. |
|from functools import reduce|입력에 차원을 곱하여 1D 크기를 계산하기 위해 사용됩니다.  |
|from operator import mul   |입력에 차원을 곱하여 1D 크기를 계산하기 위해 사용됩니다. |
|import torch| torch                  |pytorch                                    |
|from torch.nn import BatchNorm1d             | 1D 텐서 배치 정규화 |
|from torch.nn import Dropout             |    과적합 방지를 위한 드롭아웃    |
|from torch.nn import Module           |    PyTorch 모델 기본 클래스      |
|from torch.nn import ModuleList |   모델의 레이어를 리스트 형태로 저장     |
|from brevitas.nn import QuantIdentity |입력 또는 중간 출력 데이터를 양자화|
|from brevitas.nn import QuantLinear       |  양자화된 선형(fully connected) 레이어   |
|from .common import CommonActQuant      |   Brevitas에서 사용되는 양자화 방식 |
|from .common import CommonWeightQuant|  Brevitas에서 사용되는 양자화 방식   |
|from .tensor_norm import TensorNorm|   텐서 정규화를 위한 추가 레이어  |



```python

DROPOUT = 0.2 # 드롭 아웃 확률을 0.2로 설정

class FC(Module): # FC 클래스 정의 : PyTorch의 module을 상속받아 양자화된 완전 연결 신경망을 정의.

    def __init__( # 생성자
        self, # 클래스 내부에서 현재 인스턴스를 참조
        num_classes, # 분류해야 할 클래스 수.
        weight_bit_width, # 가중치의 비트 폭.
        act_bit_width, # 활성화(activation)의 비트 폭.
        in_bit_width, # 입력 데이터의 비트 폭
        in_channels, # 입력 채널 수 
        out_features, # 각 레이어의 출력 차원 리스트
        in_features=(28, 28)): # 입력 데이터의 초기 차원 
        super(FC, self).__init__() # FC 클래스의 부모캘래스의 __init__ 메서드를호출하는 구문

        self.features = ModuleList() # 레이어를  리스트로 저장. PyTorch는 리스트 내 레이어를 자동으로 추적 및 관리.
        self.features.append(QuantIdentity(act_quant=CommonActQuant, bit_width=in_bit_width)) ##
        self.features.append(Dropout(p=DROPOUT))  ## 입력 데이터를 양자화(QauntIdentity)하고 드롭아웃을 적용.
        in_features = reduce(mul, in_features) # 입력 차원 (28, 28)을 곱하여 1D 크기로 변환 (28 * 28 = 784).
        for out_features in out_features: ### QuantLinear 레이어를 추가. 입력 차원에서 출력 차원으로 매핑하며, 가중치 양자화를 적용.
            self.features.append(
                QuantLinear(
                    in_features=in_features,
                    out_features=out_features,
                    bias=False,
                    weight_bit_width=weight_bit_width,
                    weight_quant=CommonWeightQuant))###
            in_features = out_features#
            self.features.append(BatchNorm1d(num_features=in_features)) #배치 정규화
            self.features.append(QuantIdentity(act_quant=CommonActQuant, bit_width=act_bit_width)) #양자화
            self.features.append(Dropout(p=DROPOUT)) # 드롭 아웃 
        self.features.append(
            QuantLinear(
                in_features=in_features,
                out_features=num_classes,
                bias=False,
                weight_bit_width=weight_bit_width,
                weight_quant=CommonWeightQuant))
        self.features.append(TensorNorm())

        for m in self.modules(): # 
            if isinstance(m, QuantLinear):
                torch.nn.init.uniform_(m.weight.data, -1, 1) # 모든 QuantLinear 레이어의 가중치를 [-1,1] 사이의 균등 분포로 초기화 
    # 메서드
    def clip_weights(self, min_val, max_val): #
        for mod in self.features:
            if isinstance(mod, QuantLinear):
                mod.weight.data.clamp_(min_val, max_val) # 모든 QuantLinear 레이어의 가중치를 min_val과 max_val 사이로 제한

    def forward(self, x): #
        x = x.view(x.shape[0], -1)
        x = 2.0 * x - torch.tensor([1.0], device=x.device)
        for mod in self.features:
            x = mod(x)
        return x # 입력 텐서를 펼친 뒤 [-1,1]로 정규화, 모든 레이어를 순차적으로 적용하여 결과를 반환.
#fc 함수 정의
def fc(cfg): 
    weight_bit_width = cfg.getint('QUANT', 'WEIGHT_BIT_WIDTH') #
    act_bit_width = cfg.getint('QUANT', 'ACT_BIT_WIDTH')
    in_bit_width = cfg.getint('QUANT', 'IN_BIT_WIDTH')
    num_classes = cfg.getint('MODEL', 'NUM_CLASSES')
    in_channels = cfg.getint('MODEL', 'IN_CHANNELS')
    out_features = ast.literal_eval(cfg.get('MODEL', 'OUT_FEATURES')) # 설정파일(cfg)에서 모델 파라미터를 읽어옴
    net = FC( #
        weight_bit_width=weight_bit_width,
        act_bit_width=act_bit_width,
        in_bit_width=in_bit_width,
        in_channels=in_channels,
        out_features=out_features,
        num_classes=num_classes)
    return net # 실정값에 따라 FC 객체를 생성하고 반환.

```

양자화된 선형 레이어와 양자화된 활성화를 생성하는 몇 가지 도우미 함수를 사용해 네트워크 토폴로지가 구성되었음을 알 수 있습니다. 

레이어의 비트폭은 실제로 생성자에서 매개변수화되므로 이 네트워크의 1비트 가중치 및 활성화 버전을 인스턴스화해 보겠습니다. 

또한 이 네트워크에 대해 미리 학습된 가중치를 모델에 로드할 것입니다.

<br>

```python
from finn.util.test import get_test_model
lfc = get_test_model(netname = "LFC", wbits = 1, abits = 1, pretrained = True)
lfc
```

- **FINN 라이브러리** : `finn.util.test` 모듈에서 `get_test_model` 함수를 가져옵니다.

    - `get_test_model` :  
        - FINN에서 테스트 목적으로 사전 정의된 네트워크 모델을 생성하는 함수입니다.
        - 네트워크의 구조와 양자화 수준(비트 수), 가중치, 활성화 등을 설정할 수 있습니다.
    <br>

    - `get_test_model` 함수로 특정 네트워크 모델을 생성하여 변수 lfc에 저장합니다.
        - netname="LFC":
        - 네트워크 이름으로 `LFC`(Large Fully Connected)를 사용합니다.
        - LFC는 Fully Connected(완전 연결) 레이어로 구성된 네트워크입니다.
    <br>
    
- `wbits=1`:
    - 가중치(weight)의 비트 수를 1로 설정합니다.
    - 이는 이진화된 양자화를 의미하며, 가중치가 -1 또는 +1 값을 가질 수 있습니다.
    - FPGA에서 메모리 효율을 높이고 계산 복잡성을 줄이는 데 유리합니다.
    <br>

- `abits=1`:
    - 활성화(activation)의 비트 수를 1로 설정합니다.
    - 활성화 값 또한 이진화(-1 또는 +1)되며, 하드웨어 효율성을 극대화합니다.
    <br>

- `pretrained=True`:
    - 미리 학습된(pre-trained) 가중치를 로드합니다.
    - FINN이 제공하는 사전 학습된 모델로 초기화합니다.
    <br>

- 변수 `lfc`의 내용을 출력합니다.
- 생성된 LFC 모델의 구조, 설정된 파라미터(가중치,활성화 비트 수 등), 그리고 학습된 상태를 확인할 수 있습니다.
<br>

### 3.1.3. 
---
이제 훈련된 파이토치 네트워크를 인스턴스화했습니다. 

파이토치를 사용하여 네트워크를 통해 예제 MNIST 이미지를 실행해 보겠습니다.

```python
import torch # PyTorch 모듈을 가져와서 신경망의 텐서 계산에 사용합니다.
import matplotlib.pyplot as plt # Matplotlib을 사용하여 패키지에서 데이터를 불러오는 함수입니다.
from pkgutil import get_data # Python의 pkgutil 모듈을 사용하여 패키지에서 데이터를 불러오는 함수입니다. 
import onnx # ONNX 모델 형식에서 모델을 로드하고 조작하기 위한 라이브러리입니다.
import onnx.numpy_helper as nph # ONNX 텐서를 NumPy 배열로 변환하기 위한 도우미 함수 모듈입니다.
raw_i = get_data("qonnx.data", "onnx/mnist-conv/test_data_set_0/input_0.pb") # get_data 함수를 통해 qonnx.data 패키지에서 MNIST 이미지 데이터를 불러옵니다., 이미지 데이터.pb 는 protobuf 형식의 텐서 파일입니다.
input_tensor = onnx.load_tensor_from_string(raw_i) # 불러온 이미지 데이터를 ONNX 텐서로 로드합니다.
input_tensor_npy = nph.to_array(input_tensor) # ONNX 텐서를 NumPy 배열로 변환합니다.
input_tensor_pyt = torch.from_numpy(input_tensor_npy).float() # NumPy 배열을 PyTorch 텐서로 변환하고, 데이터를 float 형식으로 설정합니다. 이 텐서는 PyTorch 모델에 입력될 수 있습니다.
imgplot = plt.imshow(input_tensor_npy.reshape(28,28), cmap='gray') # 이미지를 28x28로 리쉐이프하여 시각화합니다. cmap='gray' 는 이미지를 회색조로 표시합니다.
```
![3.1](./3.1.PNG)


### 3.1.4.
---
```python
from torch.nn.functional import softmax # 1
# do forward pass in PyTorch/Brevitas
produced = lfc.forward(input_tensor_pyt).detach() #2
probabilities = softmax(produced, dim=-1).flatten() #3
probabilities #4 
```

1. **`from torch.nn.functional import softmax`**:
    - PyTorch의 `softmax` 함수를 가져옵니다. 소프트맥스는 출력 값을 확률 값으로 변환하는 데 사용되며, 주로 분류 문제에서 사용됩니다.
2. **`produced = lfc.forward(input_tensor_pyt).detach()`**:
    - `lfc.forward(input_tensor_pyt)`는 PyTorch/Brevitas 네트워크 모델에서 입력 텐서 `input_tensor_pyt`를 네트워크에 전달하여 **전방 전달**을 수행합니다.
    - `detach()`는 계산 그래프에서 이 출력을 분리하여, 이후 이 값에 대해 그래디언트 계산이 일어나지 않도록 합니다. 즉, **백워드 패스(역전파)** 에서는 이 값을 사용하지 않겠다는 의미입니다.
3. **`probabilities = softmax(produced, dim=-1).flatten()`**:
    - **`softmax(produced, dim=-1)`**: 모델의 출력인 `produced` 값을 소프트맥스 함수에 통과시켜, 이 값을 확률로 변환합니다. `dim=-1`은 마지막 차원(주로 클래스 차원)을 따라 소프트맥스를 적용한다는 의미입니다.
    - **`flatten()`**: 소프트맥스의 출력인 확률 값들을 1차원으로 평탄화(flatten)합니다. 이 경우 확률 분포가 1차원 배열로 나타나게 됩니다.
4. **`probabilities`**:
    - 최종적으로, 소프트맥스를 거친 확률 값들이 저장됩니다. 이는 각 클래스에 속할 확률을 나타내며, 모델이 예측한 각 클래스의 확률 분포를 확인할 수 있습니다.


```bash
tensor([0.1020, 0.0113, 0.4806, 0.0571, 0.0482, 0.0079, 0.0450, 0.0076, 0.1851, 0.0552])
```

```python
import numpy as np
objects = [str(x) for x in range(10)] # 1
y_pos = np.arange(len(objects)) #2
plt.bar(y_pos, probabilities, align='center', alpha=0.5) #3
plt.xticks(y_pos, objects) #4
plt.ylabel('Predicted Probability') #5
plt.title('LFC-w1a1 Predictions for Image')# 6
plt.show() #7
```

1. **`objects = [str(x) for x in range(10)]`**:
    - 숫자 `0`부터 `9`까지의 문자열 리스트를 생성합니다. 즉, MNIST 데이터셋에서 10개의 숫자 클래스에 해당하는 각 클래스 이름을 나타냅니다.
2. **`y_pos = np.arange(len(objects))`**:
    - 각 클래스에 해당하는 막대가 그래프에서 표시될 위치를 결정하는 배열을 만듭니다. `np.arange`는 0부터 9까지의 숫자를 생성하며, 이는 x축의 막대 위치를 정의합니다.
3. **`plt.bar(y_pos, probabilities, align='center', alpha=0.5)`**:
    - 막대 그래프를 생성합니다.
    - `y_pos`: 각 막대의 x축 위치.
    - `probabilities`: 각 클래스에 대한 예측 확률을 y축 값으로 사용합니다.
    - `align='center'`: 막대를 중앙에 정렬.
    - `alpha=0.5`: 그래프의 투명도를 설정.
4. **`plt.xticks(y_pos, objects)`**:
    - x축 눈금을 설정하며, 각 위치에 해당하는 클래스(숫자 0부터 9까지)를 지정합니다.
5. **`plt.ylabel('Predicted Probability')`**:
    - y축 레이블을 "Predicted Probability"로 설정합니다. 이 축은 각 클래스에 대한 네트워크의 예측 확률을 나타냅니다.
6. **`plt.title('LFC-w1a1 Predictions for Image')`**:
    - 그래프의 제목을 "LFC-w1a1 Predictions for Image"로 설정하여, 네트워크의 예측 결과를 나타냅니다.
7. **`plt.show()`**:
    - 그래프를 화면에 출력합니다.

![3.2](./3.2.PNG)

### 이해한 내용 정리 
---
`3.1. QONNX 교환 형식을 사용하여 Brevitas 네트워크를 FINN으로 가져오기` 을 학습하면서 이해한 것은 다음과 같다. 
- 3.1.1. 유틸리티 함수를 사용하여 함수 호출에 대한 소스 코드를 인쇄, Netron 사용을 위한 선언을 진행하였습니다.
<br>

- 3.1.2.학습이 되어있는 PyTorch 모델을 로드하여 분석을 진행하였고, 1비트의 가중치 및 활성화를 사용하여 테스트 모델을 생성하였습니다.
    - 생성된 모델을 lfc로 저장을 하였습니다.
<br>

- 3.1.3. 훈련된 `PyTorch` 네트워크를 인스턴스화를 진행하였고 `PyTorch`를 사용하여 MNIST 이미지를 출력해보았습니다.
<br>

- 3.1.4.  `lfc.foward` 를 수행시킨 뒤 `softmax` 를 사용하여 출력 값을 확률 값으로 변환시켰고 이는 각 클래스가 속할 확률을 나타냅니다.
<br>


# 3.2. Brevitas QONNX 내보내기 호출 및 Netron 을 통한 시각화 진행하기.
### 3.2.1.
---
Brevitas에는 QONNX 로 내보내는 기능이 내장되어있습니다.

이는 몇 가지 차이점을 제외하면 PyTorch의 일반 내보내기 기능과 유사합니다.

  1. 가중치 및 활성화 정량화는 Quant 및 BipolarQuant 노드를 사용하여 `Fake-quantization` 으로 표현됩니다.
  - `Fake-quatizaiton`은 딥러닝 모델을 양자화하기 위한 **훈련 단계** 에서 사용하는기술이며 양자화된 환경을 시뮬레이션하여 모델을 훈련시키는 방식입니다.

  2. `Avg Pooling`에 필요한 절단 작업은 `Trunc node` 로 표현됩니다. 
  - ex. 입력값 : 3.7 -> 출력값 : 3 
  - 값을 0에 가까운 정수로 자릅니다.

QONNX 에는 이 포맷으로 작업할 수 있는 도구 셋이 함께 제공됩니다. 이러한 도구는 [여기](https://github.com/fastmachinelearning/qonnx)에서 오픈 소스 프로젝트로 `Fast Machinelearing collaboration` 과 함께 유지됩니다.

### 3.2.2.
---
실제로 다음과 같이 `Brevitas` 모델에서 QONNX 를 내보내는 것은 매우 간단합니다.

```python
from brevitas.export import export_qonnx # 1 
export_onnx_path = "/tmp/LFCW1A1_qonnx.onnx" # 2
input_shape = (1, 1, 28, 28) # 3
export_qonnx(lfc, torch.randn(input_shape), export_onnx_path); # 4
```

1. **`from brevitas.export import export_qonnx`**:
    - `Brevitas` 라이브러리에서 `export_qonnx` 함수를 가져옵니다. 이 함수는 PyTorch 모델을 ONNX 형식으로 변환하는 데 사용됩니다.
2. **`export_onnx_path = "/tmp/LFCW1A1_qonnx.onnx"`**:
    - 변환된 ONNX 모델을 저장할 파일 경로를 지정합니다. 여기서는 `/tmp` 디렉토리에 `LFCW1A1_qonnx.onnx`라는 이름으로 저장됩니다.
3. **`input_shape = (1, 1, 28, 28)`**:
    - 모델의 입력 크기를 정의합니다. 이 경우, 입력 텐서는 배치 크기 1, 채널 수 1, 높이 28, 너비 28의 크기를 가집니다. MNIST 데이터셋의 흑백 이미지를 나타냅니다.
4. **`export_qonnx(lfc, torch.randn(input_shape), export_onnx_path)`**:
    - `export_qonnx` 함수를 호출하여 `lfc` 모델을 ONNX 형식으로 변환하고, `export_onnx_path`에 지정된 경로에 저장합니다.
    - `torch.randn(input_shape)`는 지정된 입력 형태의 랜덤 텐서를 생성하여 모델의 입력으로 사용합니다. 이는 모델의 구조를 정의하는 데 필요한 형식의 데이터를 제공합니다.


그 다음 Netron을 사용하여  시각화하면  다음과 같습니다.

```python
showInNetron(export_onnx_path)
```

![3.3](./3.3.PNG)

### 이해한 내용 정리 
---
`3.2. Brevitas QONNX 내보내기 호출 및 Netron 을 통한 시각화 진행하기` 을 학습하면서 이해한 것은 다음과 같다. 
- 3.2.1. Brevitas의 QONNX 내보내기 기능은 PyTorch와 유사하지만, `Fake-quantization`으로 양자화를 표현하고 Trunc node를 사용해 `Avg pooling` 절단을 처리합니다. 
<br>

- 3.2.2. `brevitas` 를 사용하여 `QONNX`를 내보내는 방법을 설명하였습니다. 
<br>

# 3.3 FINN으로 가져오기 및 QONNX를 FINN-ONNX로 변환하기
### 3.3.1.
---
먼저 내보낸 QONNX 모델에서 정리 변환을 시작합니다.

```python 
from qonnx.util.cleanup import cleanup #1
export_onnx_path_cleaned = "/tmp/LFCW1A1-qonnx-clean.onnx" #2
cleanup(export_onnx_path, out_file=export_onnx_path_cleaned) #3
```

1. **`from qonnx.util.cleanup import cleanup`**:
    - `qonnx` 패키지에서 `cleanup` 함수를 임포트합니다. 이 함수는 ONNX 모델 파일을 정리하는 데 사용됩니다.
2. **`export_onnx_path_cleaned = "/tmp/LFCW1A1-qonnx-clean.onnx"`**:
    - 정리된 ONNX 모델을 저장할 경로를 지정합니다. 여기서는 `/tmp/LFCW1A1-qonnx-clean.onnx`로 설정하였습니다.
3. **`cleanup(export_onnx_path, out_file=export_onnx_path_cleaned)`**:
    - `cleanup` 함수는 원본 ONNX 모델 파일인 `export_onnx_path`를 정리하여 새 파일 `export_onnx_path_cleaned`로 저장합니다. 이 과정에서 필요 없는 노드나 요소가 제거되고, 모델의 크기가 줄어들거나 최적화될 수 있습니다.


`Cleanup` 을 하게 되면 ONNX 모델을 훌련하고 변환하는 과정에서 생성되는 불필요한 요소들을 제거하여 모델의 크기를 줄이고 효율성을 높일 수 있습니다.

또한 정리된 모델은 메모리 사용량을 줄이고 실행 성능을 개선하는 데 도움을 줄 수 있습니다.

그 다음 Netron을 사용하여 시각화하면 다음과 같습니다.

![3.4](./3.4.PNG)

### 3.3.2.
---
이제 ModelWrapper를 사용하여 이 QONNX 모델을 FINN으로 가져옵니다.

여기서 모델을 즉시 실행하여 정확성을 확인할 수 있습니다.

```python
from qonnx.core.modelwrapper import ModelWrapper #1
import qonnx.core.onnx_exec as oxe #2
model = ModelWrapper(export_onnx_path_cleaned) #3
input_dict = {"global_in": nph.to_array(input_tensor)} #4
output_dict = oxe.execute_onnx(model, input_dict) #5
produced_qonnx = output_dict[list(output_dict.keys())[0]] #6

produced_qonnx
```

1. **`from qonnx.core.modelwrapper import ModelWrapper`**:
    - QONNX의 `ModelWrapper` 클래스를 임포트합니다. 이 클래스는 ONNX 모델을 감싸서 다양한 기능을 제공하는 역할을 합니다.
2. **`import qonnx.core.onnx_exec as oxe`** :
    - QONNX의 `onnx_exec` 모듈을 `oxe`라는 이름으로 임포트합니다. 이 모듈은 ONNX 모델을 실행하는 기능을 제공합니다.
3. **`model = ModelWrapper(export_onnx_path_cleaned)`**:
    - `export_onnx_path_cleaned` 경로에 있는 정리된 ONNX 모델을 `ModelWrapper`를 사용하여 로드합니다. `model` 변수는 이제 모델을 나타내는 객체입니다.
4. **`input_dict = {"global_in": nph.to_array(input_tensor)}`**:
    - 입력 데이터로 사용할 딕셔너리를 생성합니다. `nph.to_array(input_tensor)`를 사용하여 `input_tensor`를 NumPy 배열로 변환하고, 이 배열을 `"global_in"`이라는 키에 매핑합니다. 이는 모델의 입력 형식에 맞추기 위함입니다.
5. **`output_dict = oxe.execute_onnx(model, input_dict)`**:
    - `execute_onnx` 함수를 호출하여 ONNX 모델을 실행합니다. `model`과 `input_dict`를 인자로 전달합니다. 이 함수는 모델의 출력을 포함하는 딕셔너리를 반환합니다. 결과는 `output_dict`에 저장됩니다.
6. **`produced_qonnx = output_dict[list(output_dict.keys())[0]]`**:
    - 출력 딕셔너리에서 첫 번째 키에 해당하는 값을 가져와 `produced_qonnx` 변수에 저장합니다. 이 값은 모델의 최종 출력입니다.

```bash
array([[-1.3736125, -3.5715756,  0.1768887, -1.9529207, -2.1233053,  -3.9293835, -2.1914592, -3.9634604, -0.7772659, -1.9869976]],
      dtype=float32)
```

다음 코드를 사용하여 두 개의 배열 `produced` 와 `produced_qonnx`가 요소별로 거의 동일한지 확인할 수 있습니다. 


```python
np.isclose(produced, produced_qonnx).all()
```

```bash
True
```

### 3.3.3.
---
`QONNXtoFINN` 변환을 사용하여 모델을 FINN 내부 FINN-ONNX 표현으로 변환할 수 있습니다.

특히 모든 `Quant` 및 `BipolarQuant` 노드가 사라지고 `MultiThreshold` 노드로 변환이 됩니다.

```python
from finn.transformation.qonnx.convert_qonnx_to_finn import ConvertQONNXtoFINN #1
model = ModelWrapper(export_onnx_path_cleaned) #2

model = model.transform(ConvertQONNXtoFINN()) #3

export_onnx_path_converted = "/tmp/LFCW1A1-qonnx-converted.onnx" #4
model.save(export_onnx_path_converted) #5
```

1. **`from finn.transformation.qonnx.convert_qonnx_to_finn import ConvertQONNXtoFINN`**:
    - FINN 라이브러리에서 QONNX 모델을 FINN 모델로 변환하는 변환 클래스를 임포트합니다.
2. **`model = ModelWrapper(export_onnx_path_cleaned)`**:
    - 이전 단계에서 정리된 ONNX 모델을 `ModelWrapper`를 사용하여 로드합니다. 이 단계에서 모델이 FINN으로 변환될 준비가 됩니다.
3. **`model = model.transform(ConvertQONNXtoFINN())`**:
    - `transform` 메서드를 호출하여 QONNX 모델을 FINN 모델로 변환합니다. `ConvertQONNXtoFINN()` 클래스의 인스턴스가 변환 과정을 수행합니다. 변환된 모델은 다시 `model` 변수에 저장됩니다.
4. **`export_onnx_path_converted = "/tmp/LFCW1A1-qonnx-converted.onnx"`**:
    - 변환된 모델을 저장할 파일 경로를 설정합니다. 여기서는 `/tmp/LFCW1A1-qonnx-converted.onnx`로 지정합니다.
5. **`model.save(export_onnx_path_converted)`**:
    - `save` 메서드를 사용하여 변환된 FINN 모델을 지정된 경로에 저장합니다. 이로써 변환된 모델이 파일로 저장되어, 이후에 사용할 수 있게 됩니다.


![3.5](./3.5.PNG)

그런 뒤 다시 한 번 FINN/QONNX 엔진으로 모델을 실행할 수 있습니다.

```python
model = ModelWrapper(export_onnx_path_converted)
input_dict = {"global_in": nph.to_array(input_tensor)}
output_dict = oxe.execute_onnx(model, input_dict)
produced_finn = output_dict[list(output_dict.keys())[0]]

produced_finn
```

1. **`model = ModelWrapper(export_onnx_path_converted)`**:
    - 이전 단계에서 변환된 FINN 모델 파일인 `LFCW1A1-qonnx-converted.onnx`를 `ModelWrapper`를 사용하여 로드합니다. 이제 이 모델을 사용할 수 있게 됩니다.
2. **`input_dict = {"global_in": nph.to_array(input_tensor)}`**:
    - FINN 모델에 입력으로 제공할 딕셔너리를 생성합니다. 여기서 `nph.to_array(input_tensor)`를 사용하여 PyTorch 텐서를 NumPy 배열로 변환합니다. `global_in`이라는 키로 변환된 입력 텐서를 설정합니다.
3. **`output_dict = oxe.execute_onnx(model, input_dict)`**:
    - FINN 모델을 실행하여 출력 값을 생성합니다. `execute_onnx` 함수는 모델과 입력 딕셔너리를 받아서 모델의 출력을 계산합니다. 계산된 출력은 `output_dict`에 저장됩니다.
4. **`produced_finn = output_dict[list(output_dict.keys())[0]]`**:
    - `output_dict`에서 첫 번째 키의 값을 가져와서 `produced_finn`에 저장합니다. 이 값은 모델의 출력 결과입니다.
5. **`produced_finn`**:
    - 최종적으로 `produced_finn`을 호출하여, FINN 모델의 출력 값을 확인할 수 있습니다.

```bash
array([[-1.3736125, -3.5715756,  0.1768887, -1.9529207, -2.1233053,  -3.9293835, -2.1914592, -3.9634604, -0.7772659, -1.9869976]],
      dtype=float32)
```


다시 한 번 비교를 합니다.

```python
np.isclose(produced_qonnx, produced_finn).all()
```

```bash
True
```

변환 및 정리된 FINN 그래프가 여전히 동일한 출력을 생성한다는 것을 성공적으로 확인했으며, 이제 이 모델을 FINN에서 추가 처리에 사용할 수 있습니다.

### 이해한 내용 정리 
---
`3.3. FINN으로 가져오기 및 QONNX를 FINN-ONNX로 변환하기` 을 학습하면서 이해한 것은 다음과 같다. 
- 3.3.1. `qonnx` 의 `cleanup`을 사용하여 불필요한 요소들을 제거를 하는 방법을 알 수 있었습니다.
    - `cleanup` 함수는 ONNX 모델에서 불필요한 요소를 제거하여 크기를 줄이고 효율성을 높입니다.
<br>

- 3.3.2. `ModelWrapper`와 `execute_onnx`를 사용해 정리된 QONNX 모델을 실행하고, 입력에 따른 출력 정확성을 검증하는 방법을 알 수 있었습니다.
<br>

- 3.3.3. `QONNXtoFINN`변환을 사용해 `QONNX` 모델을 `FINN` 모델로 변환하고, 변환된 모델을 실행하여 정확성을 확인하는 방법을 알 수 있었습니다.
